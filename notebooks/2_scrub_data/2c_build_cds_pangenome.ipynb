{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "419530e0-91d6-4143-bdab-6a03d3c99abf",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db907a09-8b2d-4d6b-a351-2f60b163e939",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "\n",
    "plt.rcParams[\"figure.dpi\"] = 200\n",
    "sns.set_palette(\"deep\")\n",
    "sns.set_context(\"paper\")\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035f24cf-754f-4ee3-a6be-3bcdebde581e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831a448c-79e4-4d88-a5a4-13e470657230",
   "metadata": {},
   "source": [
    "## pangenome generation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cdd3388-2ee6-4fa2-b771-c124b7b82dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run pangenome generation functions\n",
    "# This functions will run CD-HIT and then process the output\n",
    "# into a P-gene matrix, a P-allele matrix, and a header-to-allele mapping\n",
    "\n",
    "# %run /home/schauhan/GitHub/amr_pangenome/amr_pangenome/pangenome.py\n",
    "\n",
    "import os, shutil, urllib.request, urllib.parse, urllib.error\n",
    "import subprocess as sp\n",
    "import hashlib \n",
    "import collections\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.sparse\n",
    "\n",
    "from tqdm.notebook import tqdm # added for progress bar\n",
    "\n",
    "CLUSTER_TYPES = {'cds':'C', 'noncoding':'T'}\n",
    "VARIANT_TYPES = {'allele':'A', 'upstream':'U', 'downstream':'D'}\n",
    "CLUSTER_TYPES_REV = {v:k for k,v in list(CLUSTER_TYPES.items())}\n",
    "VARIANT_TYPES_REV = {v:k for k,v in list(VARIANT_TYPES.items())}\n",
    "DNA_COMPLEMENT = {'A': 'T', 'C': 'G', 'G': 'C', 'T': 'A', \n",
    "              'W': 'W', 'S': 'S', 'R': 'Y', 'Y': 'R', \n",
    "              'M': 'K', 'K': 'M', 'N': 'N'}\n",
    "for bp in list(DNA_COMPLEMENT.keys()):\n",
    "    DNA_COMPLEMENT[bp.lower()] = DNA_COMPLEMENT[bp].lower()\n",
    "\n",
    "    \n",
    "def build_cds_pangenome(genome_faa_paths, output_dir, name='Test', \n",
    "                        cdhit_args={'-n':5, '-c':0.8}, \n",
    "                        fastasort_path=None, save_csv=True):\n",
    "    ''' \n",
    "    Constructs a pan-genome based on protein sequences with the following steps:\n",
    "    1) Merge FAA files for genomes of interest into a non-redundant list\n",
    "    2) Cluster CDS by sequence into putative genes using CD-Hit\n",
    "    3) Rename non-redundant CDS as <name>_C#A#, referring to cluster and allele number\n",
    "    4) Compile allele/gene membership into binary allele x genome and gene x genome tables\n",
    "    \n",
    "    Generates eight files within output_dir:\n",
    "    1) <name>_strain_by_allele.pickle.gz, binary allele x genome table with SparseArray structure\n",
    "    2) <name>_strain_by_gene.pickle.gz, binary gene x genome table with SparseArray structure\n",
    "    1) <name>_strain_by_allele.csv.gz, binary allele x genome table as flat file (if save_csv)\n",
    "    2) <name>_strain_by_gene.csv.gz, binary gene x genome table as flat file (if save_csv)\n",
    "    3) <name>_nr.faa, all non-redundant CDSs observed, with headers <name>_C#A#\n",
    "    4) <name>_nr.faa.cdhit.clstr, CD-Hit output file from clustering\n",
    "    5) <name>_allele_names.tsv, mapping between <name>_C#A# to original CDS headers\n",
    "    6) <name>_redundant_headers.tsv, lists of headers sharing the same CDS, with the\n",
    "        representative header relevant to #5 listed first for each group.\n",
    "    7) <name>_missing_headers.txt, lists headers for original entries missing sequences\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    genome_faa_paths : list \n",
    "        FAA files containing CDSs for genomes of interest. Genome \n",
    "        names are inferred from these FAA file paths.\n",
    "    output_dir : str\n",
    "        Path to directory to generate outputs and intermediates.\n",
    "    name : str\n",
    "        Header to prepend to all output files and allele names (default 'Test')\n",
    "    cdhit_args : dict\n",
    "        Alignment arguments to pass CD-Hit, other than -i, -o, and -d\n",
    "        (default {'-n':5, '-c':0.8})\n",
    "    fastasort_path : str\n",
    "        Path to Exonerate's fastasort binary, optionally for sorting\n",
    "        final FAA files (default None)\n",
    "    save_csv : bool\n",
    "        If true, saves allele and gene tables as csv.gz. May be limiting\n",
    "        step for very large tables (default True)\n",
    "        \n",
    "    Returns \n",
    "    -------\n",
    "    df_alleles : pd.DataFrame\n",
    "        Binary allele x genome table\n",
    "    df_genes : pd.DataFrame\n",
    "        Binary gene x genome table\n",
    "    '''\n",
    "    \n",
    "    ''' Merge FAAs into one file with non-redundant sequences '''\n",
    "    print('Identifying non-redundant CDS sequences...')\n",
    "    output_nr_faa = output_dir + '/' + name + '_nr.faa' # final non-redundant FAA files\n",
    "    output_shared_headers = output_dir + '/' + name + '_redundant_headers.tsv' # records headers that have the same sequence\n",
    "    output_missing_headers = output_dir + '/' + name + '_missing_headers.txt' # records headers without any seqeunce\n",
    "    output_nr_faa = output_nr_faa.replace('//','/')\n",
    "    output_shared_headers = output_shared_headers.replace('//','/')\n",
    "    output_missing_headers = output_missing_headers.replace('//','/')\n",
    "    non_redundant_seq_hashes, missing_headers = consolidate_seqs(\n",
    "        genome_faa_paths, output_nr_faa, output_shared_headers, output_missing_headers)\n",
    "    #maps sequence hash to headers of that sequence, in order observed\n",
    "    \n",
    "    ''' Apply CD-Hit to non-redundant CDS sequences '''\n",
    "    output_nr_faa_copy = output_nr_faa + '.cdhit' # temporary FAA copy generated by CD-Hit\n",
    "    output_nr_clstr = output_nr_faa + '.cdhit.clstr' # clusteeipxzr file generated by CD-Hit\n",
    "    cluster_with_cdhit(output_nr_faa, output_nr_faa_copy, cdhit_args)\n",
    "    \n",
    "    os.remove(output_nr_faa_copy) # delete CD-hit copied sequences\n",
    "    ############################################################################################################################################################\n",
    "    ''' Extract genes and alleles, rename unique sequences as <name>_C#A# '''\n",
    "    output_allele_names = output_dir + '/' + name + '_allele_names.tsv' # allele names vs non-redundant headers\n",
    "    output_allele_names = output_allele_names.replace('//','/')\n",
    "    header_to_allele = rename_genes_and_alleles(\n",
    "        output_nr_clstr, output_nr_faa, output_nr_faa, \n",
    "        output_allele_names, name=name, cluster_type='cds',\n",
    "        shared_headers_file=output_shared_headers,\n",
    "        fastasort_path=fastasort_path)\n",
    "    # maps original headers to short names <name>_C#A#\n",
    "    \n",
    "    ''' Process gene/allele membership into binary tables '''    \n",
    "    df_alleles, df_genes = build_genetic_feature_tables(\n",
    "        output_nr_clstr, genome_faa_paths, name,\n",
    "        cluster_type='cds', header_to_allele=header_to_allele)\n",
    "    \n",
    "    ''' Save tables as PICKLE.GZ (preserve SparseArrays) and CSV.GZ (backup flat file) '''\n",
    "    output_allele_table = output_dir + '/' + name + '_strain_by_allele'\n",
    "    output_gene_table = output_dir + '/' + name + '_strain_by_gene'\n",
    "    output_allele_table = output_allele_table.replace('//','/')\n",
    "    output_gene_table = output_gene_table.replace('//','/')\n",
    "    output_allele_csv = output_allele_table + '.csv.gz'\n",
    "    output_gene_csv = output_gene_table + '.csv.gz'\n",
    "    output_allele_pickle = output_allele_table + '.pickle.gz'\n",
    "    output_gene_pickle = output_gene_table + '.pickle.gz'\n",
    "    print('Saving', output_allele_pickle, '...')\n",
    "    df_alleles.to_pickle(output_allele_pickle)\n",
    "    print('Saving', output_gene_pickle, '...')\n",
    "    df_genes.to_pickle(output_gene_pickle)\n",
    "    if save_csv:\n",
    "        print('Saving', output_allele_csv, '...')\n",
    "        df_alleles.to_csv(output_allele_csv)\n",
    "        print('Saving', output_gene_csv, '...')\n",
    "        df_genes.to_csv(output_gene_csv)\n",
    "\n",
    "    return df_alleles, df_genes, header_to_allele\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def build_cds_nucl_pangenome(genome_data, output_dir, name='Test',\n",
    "                             allowed_features=['CDS', 'tRNA'],\n",
    "                             cdhit_args={'-n': 5, '-c':0.8}, fastasort_path=None,\n",
    "                             save_csv=True):\n",
    "    '''\n",
    "        Constructs a pan-genome based on  coding nucleic acid sequences with the following steps:\n",
    "        1) Extract coding transcripts based on FNA/GFF pairs\n",
    "        2) Cluster CDS by sequence into putative transcripts using CD-HIT-EST\n",
    "        3) Rename non-redundant transcript as <name>_T#A#, referring to transcript cluster and allele number\n",
    "        4) Compile allele/transcript membership into binary transcript allele x genome and transcript x genome tables\n",
    "\n",
    "        Generates eight files within output_dir:\n",
    "        1) <name>_strain_by_coding_nuc_allele.pickle.gz, binary allele x genome table with SparseArray structure\n",
    "        2) <name>_strain_by_coding_nuc.pickle.gz, binary gene x genome table with SparseArray structure\n",
    "        1) <name>_strain_by_coding_nuc_allele.csv.gz, binary allele x genome table as flat file (if save_csv)\n",
    "        2) <name>_strain_by_coding_nuc.csv.gz, binary gene x genome table as flat file (if save_csv)\n",
    "        3) <name>_coding_nuc_nr.fna, all non-redundant coding seqs observed, with headers <name>_T#A#\n",
    "        4) <name>_coding_nuc_nr.fna.cdhit.clstr, CD-HIT-EST output file from clustering\n",
    "        5) <name>_coding_nuc_allele_names.tsv, mapping between <name>_T#A# to original transcript headers\n",
    "        6) <name>_coding_nuc_redundant_headers.tsv, lists of headers sharing the same sequences, with the\n",
    "            representative header relevant to #5 listed first for each group.\n",
    "        7) <name>_coding_missing_headers.txt, lists headers for original entries missing sequences\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        genome_data : list\n",
    "            List of 2-tuples (genome_gff, genome_fna) for use by extract_coding()\n",
    "        output_dir : str\n",
    "            Path to directory to generate outputs and intermediates.\n",
    "        name : str\n",
    "            Header to prepend to all output files and allele names (default 'Test')\n",
    "        flanking : tuple\n",
    "            (X,Y) where X = number of nts to include from 5' end of feature,\n",
    "            and Y = number of nts to include from 3' end feature. Features\n",
    "            may be truncated by contig boundaries (default (0,0))\n",
    "        allowed_features : list\n",
    "            List of GFF feature types to extract. Default includes\n",
    "            features labeled \"CDS\" and \"tRNA\"\n",
    "        cdhit_args : dict\n",
    "            Alignment arguments to pass CD-HIT-EST, other than -i, -o, and -d\n",
    "            (default {'-n':5, '-c':0.8})\n",
    "        fastasort_path : str\n",
    "            Path to Exonerate's fastasort binary, optionally for sorting\n",
    "            final FAA files (default None)\n",
    "        save_csv : bool\n",
    "            If true, saves allele and gene tables as csv.gz. May be limiting\n",
    "            step for very large tables (default True)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        df_nuc_alleles : pd.DataFrame\n",
    "            Binary non-coding allele x genome table\n",
    "        df_nuc_genes : pd.DataFrame\n",
    "            Binary non-coding gene x genome table\n",
    "        '''\n",
    "\n",
    "    ''' Extract coding nucleid acid sequences from all genomes '''\n",
    "    print('Identifying non-redundant CDS sequences...')\n",
    "    output_nr_nuc_fna = output_dir + '/' + name + '_coding_nuc_nr.fna' #final non-redundant\n",
    "    output_shared_headers = output_dir + '/' + name + '_coding_nuc_redundant_headers.tsv' # records headers that have the same sequence\n",
    "    output_missing_headers = output_dir + '/' + name + '_coding_nuc_missing_headers.txt' # records headers without any seqeunce\n",
    "    output_nr_nuc_fna = output_nr_nuc_fna.replace('//','/')\n",
    "    output_shared_headers = output_shared_headers.replace('//','/')\n",
    "    output_missing_headers = output_missing_headers.replace('//','/')\n",
    "\n",
    "    # before we consolidate the sequences, we need to generate the coding fna files\n",
    "    print('Extracting coding-sequences...')\n",
    "    genome_coding_paths = []\n",
    "    for i, gff_fna in enumerate(genome_data):\n",
    "        '''Prepare output path'''\n",
    "        genome_gff, genome_fna = gff_fna\n",
    "        genome = __get_genome_from_filename__(genome_gff)\n",
    "        genome_dir = '/'.join(genome_gff.split('/')[:-1]) + '/' if '/' in genome_gff else ''\n",
    "        genome_nuc_dir = genome_dir + 'derived/'  # output coding sequences here\n",
    "        if not os.path.exists(genome_nuc_dir):\n",
    "            os.mkdir(genome_nuc_dir)\n",
    "        genome_nuc = genome_nuc_dir + genome + '_nuc_coding.fna'\n",
    "        '''Extract non-coding sequnces'''\n",
    "        print(i+1, genome)\n",
    "        genome_coding_paths.append(genome_nuc)\n",
    "        extract_coding_fna(genome_gff, genome_fna, genome_nuc,\n",
    "                          allowed_features=allowed_features)\n",
    "\n",
    "    # consolidate redundant sequences\n",
    "    non_redundant_seq_hashes, missing_headers = consolidate_seqs(\n",
    "    genome_coding_paths, output_nr_nuc_fna, output_shared_headers, output_missing_headers)\n",
    "\n",
    "    ''' Apply CD-Hit to non-redundant CDS sequences '''\n",
    "    output_nr_faa_copy = output_nr_nuc_fna + '.cdhit'  # temporary FAA copy generated by CD-Hit\n",
    "    output_nr_clstr = output_nr_nuc_fna + '.cdhit.clstr'  # cluster file generated by CD-Hit\n",
    "    cluster_with_cdhit(output_nr_nuc_fna, output_nr_faa_copy, cdhit_args)\n",
    "    os.remove(output_nr_faa_copy)  # delete CD-hit copied sequences\n",
    "\n",
    "    ''' Extract genes and alleles, rename unique sequences as <name>_C#A# '''\n",
    "    output_allele_names = output_dir + '/' + name + '_allele_names.tsv'  # allele names vs non-redundant headers\n",
    "    output_allele_names = output_allele_names.replace('//', '/')\n",
    "    header_to_allele = rename_genes_and_alleles(\n",
    "        output_nr_clstr, output_nr_nuc_fna, output_nr_nuc_fna,\n",
    "        output_allele_names, name=name, cluster_type='cds',\n",
    "        shared_headers_file=output_shared_headers,\n",
    "        fastasort_path=fastasort_path)\n",
    "    # maps original headers to short names <name>_C#A#\n",
    "\n",
    "    ''' Process gene/allele membership into binary tables '''\n",
    "    df_alleles, df_genes = build_genetic_feature_tables(\n",
    "        output_nr_clstr, genome_coding_paths, name,\n",
    "        cluster_type='cds', header_to_allele=header_to_allele)\n",
    "\n",
    "    ''' Save tables as PICKLE.GZ (preserve SparseArrays) and CSV.GZ (backup flat file) '''\n",
    "    output_allele_table = output_dir + '/' + name + '_strain_by_allele'\n",
    "    output_gene_table = output_dir + '/' + name + '_strain_by_gene'\n",
    "    output_allele_table = output_allele_table.replace('//', '/')\n",
    "    output_gene_table = output_gene_table.replace('//', '/')\n",
    "    output_allele_csv = output_allele_table + '.csv.gz'\n",
    "    output_gene_csv = output_gene_table + '.csv.gz'\n",
    "    output_allele_pickle = output_allele_table + '.pickle.gz'\n",
    "    output_gene_pickle = output_gene_table + '.pickle.gz'\n",
    "    print('Saving', output_allele_pickle, '...')\n",
    "    df_alleles.to_pickle(output_allele_pickle)\n",
    "    print('Saving', output_gene_pickle, '...')\n",
    "    df_genes.to_pickle(output_gene_pickle)\n",
    "    if save_csv:\n",
    "        print('Saving', output_allele_csv, '...')\n",
    "        df_alleles.to_csv(output_allele_csv)\n",
    "        print('Saving', output_gene_csv, '...')\n",
    "        df_genes.to_csv(output_gene_csv)\n",
    "\n",
    "    return df_alleles, df_genes, header_to_allele\n",
    "\n",
    "\n",
    "def build_noncoding_pangenome(genome_data, output_dir, name='Test', flanking=(0,0),\n",
    "                              allowed_features=['transcript', 'tRNA', 'rRNA', 'misc_binding'],\n",
    "                              cdhit_args={'-n':5, '-c':0.8}, fastasort_path=None, save_csv=True):\n",
    "    ''' \n",
    "    Constructs a pan-genome based on noncoding sequences with the following steps:\n",
    "    1) Extract non-coding transcripts (optionally with flanking NTs) based on FNA/GFF pairs\n",
    "    2) Cluster CDS by sequence into putative transcripts using CD-HIT-EST\n",
    "    3) Rename non-redundant transcript as <name>_T#A#, referring to transcript cluster and allele number\n",
    "    4) Compile allele/transcript membership into binary transcript allele x genome and transcript x genome tables\n",
    "    \n",
    "    Generates eight files within output_dir:\n",
    "    1) <name>_strain_by_noncoding_allele.pickle.gz, binary allele x genome table with SparseArray structure\n",
    "    2) <name>_strain_by_noncoding.pickle.gz, binary gene x genome table with SparseArray structure\n",
    "    1) <name>_strain_by_noncoding_allele.csv.gz, binary allele x genome table as flat file (if save_csv)\n",
    "    2) <name>_strain_by_noncoding.csv.gz, binary gene x genome table as flat file (if save_csv)\n",
    "    3) <name>_noncoding_nr.fna, all non-redundant non-coding seqs observed, with headers <name>_T#A#\n",
    "    4) <name>_noncoding_nr.fna.cdhit.clstr, CD-HIT-EST output file from clustering\n",
    "    5) <name>_noncoding_allele_names.tsv, mapping between <name>_T#A# to original transcript headers\n",
    "    6) <name>_noncoding_redundant_headers.tsv, lists of headers sharing the same sequences, with the\n",
    "        representative header relevant to #5 listed first for each group.\n",
    "    7) <name>_noncoding_missing_headers.txt, lists headers for original entries missing sequences\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    genome_data : list\n",
    "        List of 2-tuples (genome_gff, genome_fna) for use by extract_noncoding()\n",
    "    output_dir : str\n",
    "        Path to directory to generate outputs and intermediates.\n",
    "    name : str\n",
    "        Header to prepend to all output files and allele names (default 'Test')\n",
    "    flanking : tuple\n",
    "        (X,Y) where X = number of nts to include from 5' end of feature,\n",
    "        and Y = number of nts to include from 3' end feature. Features\n",
    "        may be truncated by contig boundaries (default (0,0))\n",
    "    allowed_features : list\n",
    "        List of GFF feature types to extract. Default excludes \n",
    "        features labeled \"CDS\" or \"repeat_region\" \n",
    "        (default ['transcript', 'tRNA', 'rRNA', 'misc_binding'])\n",
    "    cdhit_args : dict\n",
    "        Alignment arguments to pass CD-HIT-EST, other than -i, -o, and -d\n",
    "        (default {'-n':5, '-c':0.8})\n",
    "    fastasort_path : str\n",
    "        Path to Exonerate's fastasort binary, optionally for sorting\n",
    "        final FAA files (default None)\n",
    "    save_csv : bool\n",
    "        If true, saves allele and gene tables as csv.gz. May be limiting\n",
    "        step for very large tables (default True)\n",
    "        \n",
    "    Returns \n",
    "    -------\n",
    "    df_nc_alleles : pd.DataFrame\n",
    "        Binary non-coding allele x genome table\n",
    "    df_nc_genes : pd.DataFrame\n",
    "        Binary non-coding gene x genome table\n",
    "    '''\n",
    "    \n",
    "    ''' Extract non-coding sequences from all genomes '''\n",
    "    print('Extracting non-coding sequences...')\n",
    "    genome_noncoding_paths = []\n",
    "    for i, gff_fna in enumerate(genome_data):\n",
    "        ''' Prepare output path '''\n",
    "        genome_gff, genome_fna = gff_fna\n",
    "        genome = __get_genome_from_filename__(genome_gff)\n",
    "        genome_dir = '/'.join(genome_gff.split('/')[:-1]) + '/' if '/' in genome_gff else ''\n",
    "        genome_nc_dir = genome_dir + 'derived/' # output noncoding sequences here\n",
    "        if not os.path.exists(genome_nc_dir):\n",
    "            os.mkdir(genome_nc_dir)\n",
    "        genome_nc = genome_nc_dir + genome + '_noncoding.fna'\n",
    "            \n",
    "        ''' Extract non-coding sequences '''\n",
    "        print(i+1, genome)\n",
    "        genome_noncoding_paths.append(genome_nc)\n",
    "        extract_noncoding(genome_gff, genome_fna, genome_nc, \n",
    "            flanking=flanking, allowed_features=allowed_features)\n",
    "        \n",
    "    ''' Reduce to non-redundant sequence set '''\n",
    "    print('Identifying non-redundant non-coding sequences...')\n",
    "    output_nr_fna = output_dir + '/' + name + '_noncoding_nr.fna' # final non-redundant FNA files\n",
    "    output_shared_headers = output_dir + '/' + name + '_noncoding_redundant_headers.tsv' \n",
    "        # records headers that have the same sequence\n",
    "    output_missing_headers = output_dir + '/' + name + '_noncoding_missing_headers.txt' \n",
    "        # records headers without any seqeunce\n",
    "    output_nr_fna = output_nr_fna.replace('//','/')\n",
    "    output_shared_headers = output_shared_headers.replace('//','/')\n",
    "    output_missing_headers = output_missing_headers.replace('//','/')\n",
    "    nr_seq_hashes, missing_headers = consolidate_seqs(\n",
    "        genome_noncoding_paths, output_nr_fna, \n",
    "        output_shared_headers, output_missing_headers)\n",
    "    # maps sequence hash to headers of that sequence, in order observed\n",
    "    ''' Apply CD-Hit to non-redundant non-coding sequences '''\n",
    "    output_nr_fna_copy = output_nr_fna + '.cdhit' # temporary FNA copy generated by CD-HIT-EST\n",
    "    output_nr_clstr = output_nr_fna + '.cdhit.clstr' # cluster file generated by CD-HIT-EST\n",
    "    cluster_with_cdhit(output_nr_fna, output_nr_fna_copy, cdhit_args)\n",
    "    os.remove(output_nr_fna_copy) # delete CD-HIT-EST copied sequences\n",
    "    \n",
    "    ''' Extract genes and alleles, rename unique sequences as <name>_T#A# '''\n",
    "    output_allele_names = output_dir + '/' + name + '_noncoding_allele_names.tsv' # allele names vs non-redundant headers\n",
    "    output_allele_names = output_allele_names.replace('//','/')\n",
    "    header_to_allele = rename_genes_and_alleles(\n",
    "        output_nr_clstr, output_nr_fna, output_nr_fna, \n",
    "        output_allele_names, name=name, cluster_type='noncoding',\n",
    "        shared_headers_file=output_shared_headers,\n",
    "        fastasort_path=fastasort_path)\n",
    "    # maps original headers to short names <name>_T#A#\n",
    "    \n",
    "    ''' Process gene/allele membership into binary tables '''    \n",
    "    df_nc_alleles, df_nc_genes = build_genetic_feature_tables(\n",
    "        output_nr_clstr, genome_noncoding_paths, name, \n",
    "        cluster_type='noncoding', header_to_allele=header_to_allele)\n",
    "    df_nc_alleles.columns = df_nc_alleles.columns.map(lambda x: x.replace('_noncoding',''))\n",
    "    df_nc_genes.columns = df_nc_genes.columns.map(lambda x: x.replace('_noncoding','')) \n",
    "    \n",
    "    ''' Save tables as PICKLE.GZ (preserve SparseArrays) and CSV.GZ (backup flat file) '''\n",
    "    output_allele_table = output_dir + '/' + name + '_strain_by_noncoding_allele'\n",
    "    output_gene_table = output_dir + '/' + name + '_strain_by_noncoding_gene'\n",
    "    output_allele_table = output_allele_table.replace('//','/')\n",
    "    output_gene_table = output_gene_table.replace('//','/')\n",
    "    output_allele_csv = output_allele_table + '.csv.gz'\n",
    "    output_gene_csv = output_gene_table + '.csv.gz'\n",
    "    output_allele_pickle = output_allele_table + '.pickle.gz'\n",
    "    output_gene_pickle = output_gene_table + '.pickle.gz'\n",
    "    print('Saving', output_allele_pickle, '...')\n",
    "    df_nc_alleles.to_pickle(output_allele_pickle)\n",
    "    print('Saving', output_gene_pickle, '...')\n",
    "    df_nc_genes.to_pickle(output_gene_pickle)\n",
    "    if save_csv:\n",
    "        print('Saving', output_allele_csv, '...')\n",
    "        df_nc_alleles.to_csv(output_allele_csv)\n",
    "        print('Saving', output_gene_csv, '...')\n",
    "        df_nc_genes.to_csv(output_gene_csv)\n",
    "    \n",
    "    return df_nc_alleles, df_nc_genes, header_to_allele\n",
    "    \n",
    "\n",
    "def consolidate_seqs(genome_paths, nr_out, shared_headers_out, missing_headers_out=None):\n",
    "    ''' \n",
    "    Combines sequences for many genomes into a single file without duplicate\n",
    "    sequences to be clustered using CD-Hit, i.e. with cluster_with_cdhit(). Tracks\n",
    "    headers that share the same sequence, and optionally headers without sequences.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    genome_paths : list\n",
    "        Paths to genome FAA/FNA files to combine\n",
    "    nr_out : str\n",
    "        Output path for combined non-redundant FAA/FNA file\n",
    "    shared_headers_out : str\n",
    "        Output path for shared headers TSV file\n",
    "    missing_headers_out : str\n",
    "        Output path for headers without sequences TXT file (default None)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    non_redundant_seq_hashes : dict\n",
    "        Maps non-redundant sequence hashes to a list of headers, in order observed\n",
    "    missing_headers : list\n",
    "        List of headers without any associated sequence\n",
    "    '''\n",
    "    non_redundant_seq_hashes = {} # maps sequence hash to headers of that sequence, in order observed\n",
    "    encounter_order = [] # stores sequence hashes in order encountered\n",
    "    missing_headers = [] # stores headers without sequences\n",
    "    \n",
    "    def process_header_and_seq(header, seq_blocks, output_file):\n",
    "        ''' Processes a header/sequence pair against the running list of non-redundant sequences '''\n",
    "        seq = ''.join(seq_blocks)\n",
    "        seq = seq.encode('utf-8')\n",
    "        if len(header) > 0 and len(seq) > 0: # valid header-sequence record\n",
    "            seqhash = __hash_sequence__(seq)\n",
    "            if seqhash in non_redundant_seq_hashes: # record repeated appearances of sequence\n",
    "                non_redundant_seq_hashes[seqhash].append(header)\n",
    "            else: # first encounter of a sequence, record to non-redundant file\n",
    "                encounter_order.append(seqhash)\n",
    "                non_redundant_seq_hashes[seqhash] = [header]\n",
    "                output_file.write('>' + header + '\\n')\n",
    "                output_file.write('\\n'.join(seq_blocks) + '\\n')\n",
    "        elif len(header) > 0 and len(seq) == 0: # header without sequence\n",
    "            missing_headers.append(header)\n",
    "    \n",
    "    ''' Scan for redundant sequences across all files, build non-redundant file '''\n",
    "    with open(nr_out, 'w+') as f_nr_out:\n",
    "        for genome_path in genome_paths:\n",
    "            with open(genome_path, 'r') as f:\n",
    "                header = ''; seq_blocks = []\n",
    "                for line in f:\n",
    "                    if line[0] == '>': # header encountered\n",
    "                        process_header_and_seq(header, seq_blocks, f_nr_out)\n",
    "\n",
    "                        header = __get_header_from_fasta_line__(line)\n",
    "                        seq_blocks = []\n",
    "                    else: # sequence line encountered\n",
    "                        seq_blocks.append(line.strip())\n",
    "\n",
    "                process_header_and_seq(header, seq_blocks, f_nr_out) # process last record\n",
    "                \n",
    "    ''' Save shared and missing headers to file '''\n",
    "    with open(shared_headers_out, 'w+') as f_header_out:\n",
    "        for seqhash in encounter_order:\n",
    "            headers = non_redundant_seq_hashes[seqhash]\n",
    "            if len(headers) > 1:\n",
    "                f_header_out.write('\\t'.join(headers) + '\\n')\n",
    "    if missing_headers_out:\n",
    "        print('Headers without sequences:', len(missing_headers))\n",
    "        with open(missing_headers_out, 'w+') as f_header_out:\n",
    "            for header in missing_headers:\n",
    "                f_header_out.write(header + '\\n')\n",
    "                \n",
    "    return non_redundant_seq_hashes, missing_headers\n",
    "        \n",
    "                \n",
    "def cluster_with_cdhit(fasta_file, cdhit_out, cdhit_args={'-n':5, '-c':0.8}):\n",
    "    '''\n",
    "    Runs CD-Hit on a fasta file, i.e. one generated by consolidate_seqs().\n",
    "    Requires CD-Hit to be available in PATH. Uses CD-HIT for FAA files (default),\n",
    "    and CD-HIT-EST for FNA files. \n",
    "    \n",
    "    For CD-HIT-EST, word size \"-n\" has a more direct impact of similarity threshold\n",
    "    See https://github.com/weizhongli/cdhit/wiki/3.-User's-Guide#CDHITEST\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    fasta_file : str\n",
    "        Path to FAA/FNA file to be clustered, i.e. from consolidate_seqs()\n",
    "    cdhit_out : str\n",
    "        Path to be provided to CD-Hit output argument\n",
    "    cdhit_args : dict\n",
    "        Dictionary of alignment arguments to be provided to CD-Hit, other than\n",
    "        -i, -o, and -d. Default is for FAA files. (default {'-n':5, '-c':0.8})\n",
    "    ''' \n",
    "    cdhit_prog = 'cd-hit-est' if fasta_file[-4:].lower() == '.fna' else 'cd-hit'\n",
    "    args = [cdhit_prog, '-i', fasta_file, '-o', cdhit_out, '-d', '0']\n",
    "    for arg in cdhit_args:\n",
    "        args += [arg, str(cdhit_args[arg])]\n",
    "    print('Running:', args)\n",
    "    for line in __stream_stdout__(' '.join(args)):\n",
    "        print(line)\n",
    "\n",
    "        \n",
    "def rename_genes_and_alleles(clstr_file, nr_fasta_in, nr_fasta_out, \n",
    "                             feature_names_out, name='Test', cluster_type='cds',\n",
    "                             shared_headers_file=None, fastasort_path=None):\n",
    "    '''\n",
    "    Processes a CD-Hit CLSTR file (clstr_file) to rename headers in the orignal\n",
    "    fasta as <name>_C#A# for CDS, or <name>_T#A# for non-coding features,\n",
    "    based on cluster membership and stores header-name mappings as a TSV.\n",
    "    \n",
    "    Can optionally sort final fasta file if fastasort_path is specified, from Exonerate\n",
    "    https://www.ebi.ac.uk/about/vertebrate-genomics/software/exonerate\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    clstr_file : str\n",
    "        Path to CLSTR file generated by CD-Hit\n",
    "    nr_fasta_in : str\n",
    "        Path to FAA/FNA file corresponding to clstr_file\n",
    "    nr_fasta_out : str\n",
    "        Output path for renamed FAA/FNA, will overwrite if equal to nr_fasta_in\n",
    "    feature_names_out : str\n",
    "        Output path for header-allele name mapping TSV file\n",
    "    name : str\n",
    "        Header to append output files and allele names (default 'Test')\n",
    "    cluster_type : str\n",
    "        If 'cds', features are named <name>_C#A# for gene clusters/alleles. \n",
    "        If 'noncoding', features are named <name>_T#A# for transcripts (default 'cds')\n",
    "    shared_headers_file : str\n",
    "        Path to shared headers. If provided, will expand the header-allele\n",
    "        mapping to include headers that map to the same sequence/allele (default None)\n",
    "    fastasort_path : str\n",
    "        Path to Exonerate fastasort, used to optionally sort nr_faa (default None)\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    header_to_allele : dict\n",
    "        Maps original headers to new allele names\n",
    "    '''\n",
    "    \n",
    "    ''' Optionally, load up shared headers '''\n",
    "    shared_headers = {} # maps representative header to synonym headers\n",
    "    if shared_headers_file:\n",
    "        with open(shared_headers_file, 'r') as f_share:\n",
    "            for line in f_share:\n",
    "                headers = line.strip().split('\\t')\n",
    "                representative_header = headers[0]\n",
    "                synonym_headers = headers[1:]\n",
    "                shared_headers[representative_header] = synonym_headers\n",
    "    \n",
    "    ''' Read through CLSTR file to map original headers to C#A#/T#A# names '''\n",
    "    header_to_allele = {} # maps headers to allele name (name_C#A# or name_T#A#)\n",
    "    max_cluster = 0 \n",
    "    with open(feature_names_out, 'w+') as f_naming:\n",
    "        with open(clstr_file, 'r') as f_clstr:\n",
    "            for line in tqdm(f_clstr):\n",
    "                if line[0] == '>': # starting new gene cluster\n",
    "                    cluster_num = line.split()[-1].strip() # cluster number as string\n",
    "                    max_cluster = cluster_num\n",
    "                    if cluster_num == '78264':\n",
    "                        print(cluster_num)\n",
    "                else: # adding allele to cluster\n",
    "                    if cluster_num == '78264':\n",
    "                        print(line.split()[2])\n",
    "                    data = line.split()\n",
    "                    allele_num = int(data[0]) # allele number as string, convert to int\n",
    "                    allele_header = data[2][1:-3] # old allele header\n",
    "                    allele_name = create_feature_name(name, cluster_type, cluster_num, 'allele', allele_num)\n",
    "                    header_to_allele[allele_header] = allele_name\n",
    "                    mapped_headers = [allele_header]\n",
    "                    if allele_header in shared_headers: # if synonym headers are available\n",
    "                        for synonym_header in shared_headers[allele_header]:\n",
    "                            header_to_allele[synonym_header] = allele_name\n",
    "                        mapped_headers += shared_headers[allele_header]\n",
    "                    f_naming.write(allele_name + '\\t' + ('\\t'.join(mapped_headers)).strip() + '\\n')\n",
    "                    \n",
    "    ''' Create the fasta file with renamed features '''\n",
    "    with open(nr_fasta_in, 'r') as f_fasta_old:\n",
    "        with open(nr_fasta_out + '.tmp', 'w+') as f_fasta_new:\n",
    "            ''' Iterate through alleles in cluster/allele order '''\n",
    "            missing = True # if currently in a sequence without a header\n",
    "            for line in tqdm(f_fasta_old):\n",
    "                if line[0] == '>': # writing updated header line\n",
    "                    allele_header = line[1:].strip()\n",
    "                    if allele_header in header_to_allele:\n",
    "                        allele_name = header_to_allele[allele_header]\n",
    "                        f_fasta_new.write('>' + allele_name + '\\n')\n",
    "                        missing = False\n",
    "                    else:\n",
    "                        print('MISSING:', allele_header)\n",
    "                        missing = True\n",
    "                elif not missing: # writing sequence line\n",
    "                    f_fasta_new.write(line)\n",
    "    \n",
    "    ''' Move fasta file to desired output path '''\n",
    "    if nr_fasta_out == nr_fasta_in: # if overwriting, remove old faa file\n",
    "        os.remove(nr_fasta_in) \n",
    "    os.rename(nr_fasta_out + '.tmp', nr_fasta_out)\n",
    "    \n",
    "    ''' If available, use exonerate.fastasort to sort entries in fasta file '''\n",
    "    if fastasort_path:\n",
    "        print('Sorting sequences by header...')\n",
    "        args = [fastasort_path, nr_fasta_out]\n",
    "        with open(nr_fasta_out + '.tmp', 'w+') as f_sort:\n",
    "            #sp.call(args, stdout=f_sort)\n",
    "            p = sp.Popen(args, stdout=f_sort, stderr=sp.PIPE)\n",
    "            stdout, stderr = p.communicate()\n",
    "            print(stderr)\n",
    "        if p.returncode == 1: # sorting failed\n",
    "            print('Aborting sort, exitcode', p.returncode)\n",
    "            os.remove(nr_fasta_out + '.tmp')\n",
    "        else: # sorting passed (probably)\n",
    "            os.rename(nr_fasta_out + '.tmp', nr_fasta_out)\n",
    "    return header_to_allele\n",
    "\n",
    "\n",
    "def build_genetic_feature_tables(clstr_file, genome_fasta_paths, name='Test', cluster_type='cds',\n",
    "                                 shared_header_file=None, header_to_allele=None):\n",
    "    '''\n",
    "    Builds two binary tables based on the presence/absence of genetic features, \n",
    "    allele x genome (allele_table_out) and gene x genome (gene_table_out).\n",
    "    Works for both CDS clusters and non-coding transcript clusters.\n",
    "    Uses a CD-Hit CLSTR file, the corresponding original fasta files, \n",
    "    and shared header mappings.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    clstr_file : str\n",
    "        Path to CD-Hit CLSTR file used to build header-allele mappings\n",
    "    genome_fasta_paths : list\n",
    "        Paths to genome fasta files originally combined and clustered (see consolidate_seqs)\n",
    "    name : str\n",
    "        Name to attach to features and files (default 'Test')\n",
    "    cluster_type : str\n",
    "        If 'cds', features are named <name>_C#A# for gene clusters/alleles. \n",
    "        If 'noncoding', features are named <name>_T#A# for transcripts (default 'cds')\n",
    "    shared_header_file : str\n",
    "        Path to shared header TSV file, if synonym headers are not mapped\n",
    "        in header_to_allele or header_to_allele is not provided (default None)\n",
    "    header_to_allele : dict\n",
    "        Pre-calculated header-allele mappings corresponding to clstr_file,\n",
    "        if available from rename_genes_and_alleles() (default None)\n",
    "\n",
    "    Returns \n",
    "    -------\n",
    "    df_alleles : pd.DataFrame\n",
    "        Binary allele x genome table\n",
    "    df_genes : pd.DataFrame\n",
    "        Binary gene x genome table\n",
    "    '''\n",
    "    \n",
    "    ''' Load header-allele mappings '''\n",
    "    print('Loadings header-allele mappings...')\n",
    "    header_to_allele = load_header_to_allele(clstr_file, \n",
    "        shared_header_file, header_to_allele, cluster_type)\n",
    "                    \n",
    "    ''' Initialize gene and allele tables '''\n",
    "    genome_order = sorted([__get_genome_from_filename__(x) for x in genome_fasta_paths]) \n",
    "        # for genome names, trim .faa from filenames\n",
    "    print('Sorting alleles...')\n",
    "    allele_order = sorted(list(set(header_to_allele.values())))\n",
    "    \n",
    "    print('Sorting clusters...')\n",
    "    gene_order = []; last_gene = None\n",
    "    for allele in tqdm(allele_order):\n",
    "        gene = __get_gene_from_allele__(allele)\n",
    "        if gene != last_gene:\n",
    "            gene_order.append(gene)\n",
    "            last_gene = gene\n",
    "    print('Genomes:', len(genome_order))\n",
    "    print('Clusters:', len(gene_order))\n",
    "    print('Alleles:', len(allele_order))\n",
    "    \n",
    "    ''' To use sparse matrix, map genomes, alleles, and genes to positions '''\n",
    "    allele_indices = {allele_order[i]:i for i in range(len(allele_order))}\n",
    "    gene_indices = {gene_order[i]:i for i in range(len(gene_order))}   \n",
    "    allele_arrays = {} # maps genome:allele vectors as SparseArrays\n",
    "    gene_arrays = {} # maps genome:gene vector as SparseArrays\n",
    "\n",
    "    ''' Scan original genome file for allele and gene membership '''\n",
    "    for i, genome_fasta in enumerate(sorted(genome_fasta_paths)):\n",
    "        genome = __get_genome_from_filename__(genome_fasta)\n",
    "        genome_i = genome_order.index(genome)\n",
    "        allele_arrays[genome] = np.zeros(shape=len(allele_order), dtype='int64')\n",
    "        gene_arrays[genome] = np.zeros(shape=len(gene_order), dtype='int64')\n",
    "        with open(genome_fasta, 'r') as f_fasta:\n",
    "            header = ''; seq = '' # track the sequence to skip over empty sequences\n",
    "            for line in f_fasta.readlines(): # pre-load, slight speed-up\n",
    "                ''' Load all alleles and genes per genome '''\n",
    "                if line[0] == '>': # new header line encountered\n",
    "                    if len(seq) > 0:\n",
    "                        if header in header_to_allele:\n",
    "                            allele_name = header_to_allele[header]\n",
    "                            allele_i = allele_indices[allele_name]\n",
    "                            allele_arrays[genome][allele_i] = 1\n",
    "                            gene = __get_gene_from_allele__(allele_name)\n",
    "                            gene_i = gene_indices[gene]\n",
    "                            gene_arrays[genome][gene_i] = 1\n",
    "                        else:\n",
    "                            print('MISSING:', header)\n",
    "                    header = __get_header_from_fasta_line__(line)\n",
    "                    seq = '' # reset sequence\n",
    "                else: # sequence line encountered\n",
    "                    seq += line.strip()\n",
    "            if len(seq) > 0: # process last record\n",
    "                if header in header_to_allele:\n",
    "                    allele_name = header_to_allele[header]\n",
    "                    allele_i = allele_indices[allele_name]\n",
    "                    allele_arrays[genome][allele_i] = 1\n",
    "                    gene = __get_gene_from_allele__(allele_name)\n",
    "                    gene_i = gene_indices[gene]\n",
    "                    gene_arrays[genome][gene_i] = 1\n",
    "                else:\n",
    "                    print('MISSING:', header)\n",
    "                \n",
    "        allele_arrays[genome] = pd.arrays.SparseArray(allele_arrays[genome])\n",
    "        gene_arrays[genome] = pd.arrays.SparseArray(gene_arrays[genome])\n",
    "        allele_arrays[genome].fill_value = np.nan\n",
    "        gene_arrays[genome].fill_value = np.nan\n",
    "        print('Updating genome', i+1, ':', genome, end=' ') \n",
    "        print('\\tAlleles:', allele_arrays[genome].sum(), '\\tClusters:', gene_arrays[genome].sum())\n",
    "        \n",
    "    ''' Construct DataFrame '''\n",
    "    print('Building DataFrame...')\n",
    "    df_alleles = pd.DataFrame(data=allele_arrays, index=allele_order)\n",
    "    df_genes = pd.DataFrame(data=gene_arrays, index=gene_order)\n",
    "    return df_alleles, df_genes\n",
    "\n",
    "\n",
    "def load_header_to_allele(clstr_file=None, shared_header_file=None, \n",
    "                          header_to_allele=None, name='Test', cluster_type='cds'):\n",
    "    '''\n",
    "    Loads a mapping from original fasta headers to allele names format \n",
    "    <name>_C#A# for genes or <name>_T#A# for noncoding features.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    clstr_file : str\n",
    "        Path to CD-Hit CLSTR file used to build header-allele mappings,\n",
    "        only needed if header_to_allele is None (default None)\n",
    "    shared_header_file : str\n",
    "        Path to shared header TSV file, if synonym headers are not mapped\n",
    "        in header_to_allele or header_to_allele is not provided (default None)\n",
    "    header_to_allele : dict\n",
    "        Pre-calculated header-allele mappings corresponding to clstr_file,\n",
    "        if available from rename_genes_and_alleles (default None)\n",
    "    name : str\n",
    "        Name to attach to features and files (default 'Test')\n",
    "    cluster_type : str\n",
    "        If 'cds', features are named <name>_C#A# for gene clusters/alleles. \n",
    "        If 'noncoding', features are named <name>_T#A# for transcripts (default 'cds')\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    full_header_to_allele : dict\n",
    "        Full header-allele mappings combining contents of both header_to_allele \n",
    "        (copied or built from clstr_file) and shared_header_file.\n",
    "    '''\n",
    "    \n",
    "    ''' Load header to allele mapping from CLSTR, if not provided '''\n",
    "    if header_to_allele is None:\n",
    "        full_header_to_allele = {} # maps representative header to allele name (name_C#A#)\n",
    "        with open(clstr_file, 'r') as f_clstr:\n",
    "            for line in f_clstr:\n",
    "                if line[0] == '>': # starting new gene cluster\n",
    "                    cluster_num = line.split()[-1].strip() # cluster number as string\n",
    "                    max_cluster = cluster_num\n",
    "                    if cluster_num == '78264':\n",
    "                        print(cluster_num)\n",
    "                else: # adding allele to cluster\n",
    "                    if cluster_num == '78264':\n",
    "                        print(line.split()[2])\n",
    "                    data = line.split()\n",
    "                    allele_num = data[0] # allele number as string\n",
    "                    allele_header = data[2][1:-3] # old allele header\n",
    "                    allele_name = create_feature_name(name, cluster_type, cluster_num, 'allele', allele_num)\n",
    "                    full_header_to_allele[allele_header] = allele_name\n",
    "    elif type(header_to_allele) == dict:\n",
    "        full_header_to_allele = header_to_allele.copy()\n",
    "    \n",
    "    ''' Load headers that share the same sequence '''\n",
    "    if shared_header_file:\n",
    "        with open(shared_header_file, 'r') as f_header:\n",
    "            for line in f_header:\n",
    "                headers = [x.strip() for x in line.split('\\t')]\n",
    "                if len(headers) > 1:\n",
    "                    repr_header = headers[0]\n",
    "                    repr_allele = full_header_to_allele[repr_header]\n",
    "                    for alt_header in headers[1:]:\n",
    "                        full_header_to_allele[alt_header] = repr_allele\n",
    "    return full_header_to_allele\n",
    "\n",
    "\n",
    "def build_upstream_pangenome(genome_data, allele_names, output_dir, limits=(-50,3), \n",
    "                             name='Test', include_fragments=False, max_overlap=-1, \n",
    "                             fastasort_path=None, save_csv=True):\n",
    "    '''\n",
    "    Extracts nucleotides upstream of coding sequences for multiple genomes, \n",
    "    create <genome>_upstream.fna files in the same directory for each genome.\n",
    "    Then, classifies/names them relative to gene clusters identified by coding sequence,  \n",
    "    i.e. after build_cds_pangenome(). See build_proximal_pangenome() for parameters.\n",
    "    '''\n",
    "    return build_proximal_pangenome(\n",
    "        genome_data, allele_names, output_dir, limits, \n",
    "        side='upstream', name=name, include_fragments=include_fragments, \n",
    "        max_overlap=max_overlap, fastasort_path=fastasort_path, save_csv=save_csv)\n",
    "    \n",
    "\n",
    "def build_downstream_pangenome(genome_data, allele_names, output_dir, limits=(-3,50), \n",
    "                               name='Test', include_fragments=False, max_overlap=-1, \n",
    "                               fastasort_path=None, save_csv=True):\n",
    "    '''\n",
    "    Extracts nucleotides downstream of coding sequences for multiple genomes, \n",
    "    create <genome>_downstream.fna files in the same directory for each genome.\n",
    "    Then, classifies/names them relative to gene clusters identified by coding sequence,  \n",
    "    i.e. after build_cds_pangenome(). See build_proximal_pangenome() for parameters.\n",
    "    '''\n",
    "    return build_proximal_pangenome(\n",
    "        genome_data, allele_names, output_dir, limits, \n",
    "        side='downstream', name=name, include_fragments=include_fragments, \n",
    "        max_overlap=max_overlap, fastasort_path=fastasort_path, save_csv=save_csv)\n",
    "\n",
    "    \n",
    "def build_proximal_pangenome(genome_data, allele_names, output_dir, limits, side, name='Test', \n",
    "                             include_fragments=False, max_overlap=-1, fastasort_path=None, save_csv=True):\n",
    "    '''\n",
    "    Extracts nucleotides proximal to coding sequences for multiple genomes, \n",
    "    create genome-specific proximal sequence fna files in the same directory for each genome.\n",
    "    Then, classifies/names them relative to gene clusters identified by coding sequence,  \n",
    "    i.e. after build_cds_pangenome(). See extract_proximal_sequences() and\n",
    "    consolidate_proximal() for more details.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    genome_data : list\n",
    "        List of 2-tuples (genome_gff, genome_fna) for use by extract_proximal_sequences()\n",
    "    allele_names : str\n",
    "        Path to allele names file, should be named <name>_allele_names.tsv\n",
    "    output_dir : str\n",
    "        Path to directory to generate summary outputs.\n",
    "    limits : 2-tuple\n",
    "        Length of proximal region to extract, formatted (-X,Y). For upstream, extracts X \n",
    "        upstream bases (up to but excluding first base of start codon) and first Y coding \n",
    "        bases (including first base of start codon). For downstream, extracts the last X\n",
    "        coding bases and Y downstream bases. In both cases, the total length is X+Y bases.\n",
    "    side : str\n",
    "        'upstream' or 'downstream' for 5'UTR or 3'UTR\n",
    "    name : str\n",
    "        Short header to prepend output summary files, recommendated to be same as what\n",
    "        was used in the build_cds_pangenome() (default 'Test')\n",
    "    include_fragments : bool\n",
    "        If true, include proximal sequences that are not fully available \n",
    "        due to contig boundaries (default False)\n",
    "    max_overlap : int\n",
    "        If non-negative, truncates UTRs that cross over into coding sequences\n",
    "        of other genes such that the overlap is no more than <max_overlap> nts.\n",
    "        If negative, does not truncate UTRs s.t. all UTRs same length (default -1)\n",
    "    fastasort_path : str\n",
    "        Path to Exonerate's fastasort binary, optionally for sorting\n",
    "        final FNA files (default None)\n",
    "    save_csv : bool\n",
    "        If true, saves allele and gene tables as csv.gz. May be limiting\n",
    "        step for very large tables (default True)\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    df_proximal : pd.DataFrame\n",
    "        Binary proximal x genome table\n",
    "    '''\n",
    "    \n",
    "    ''' Load header-allele name mapping '''\n",
    "    print('Loading header-allele mapping...')\n",
    "    feature_to_allele = __load_feature_to_allele__(allele_names)\n",
    "        \n",
    "    ''' Generate proximal sequences '''\n",
    "    print('Extracting', side, 'sequences...')\n",
    "    genome_proximals = []\n",
    "    for i, gff_fna in enumerate(genome_data):\n",
    "        ''' Prepare output path '''\n",
    "        genome_gff, genome_fna = gff_fna\n",
    "        genome = __get_genome_from_filename__(genome_gff)\n",
    "        genome_dir = '/'.join(genome_gff.split('/')[:-1]) + '/' if '/' in genome_gff else ''\n",
    "        genome_prox_dir = genome_dir + 'derived/'\n",
    "        if not os.path.exists(genome_prox_dir):\n",
    "            os.mkdir(genome_prox_dir)\n",
    "        genome_prox = genome_prox_dir + genome + '_' + side + '.fna'\n",
    "            \n",
    "        ''' Extract proximal sequences '''\n",
    "        print(i+1, genome)\n",
    "        genome_proximals.append(genome_prox)\n",
    "        extract_proximal_sequences(genome_gff, genome_fna, genome_prox, \n",
    "                                   limits=limits, side=side,\n",
    "                                   feature_to_allele=feature_to_allele,\n",
    "                                   include_fragments=include_fragments,\n",
    "                                   max_overlap=max_overlap)\n",
    "        \n",
    "    ''' Consolidate non-redundant proximal sequences per gene '''\n",
    "    print('Identifying non-redundant', side, 'sequences per gene...')\n",
    "    nr_prox_out = output_dir + '/' + name + '_nr_' + side + '.fna'\n",
    "    nr_prox_out = nr_prox_out.replace('//','/')\n",
    "    df_proximal = consolidate_proximal(genome_proximals, nr_prox_out, feature_to_allele, side)\n",
    "    \n",
    "    ''' Optionally sort non-redundant proximal sequences file '''\n",
    "    if fastasort_path:\n",
    "        print('Sorting sequences by header...')\n",
    "        args = ['./' + fastasort_path, nr_prox_out]\n",
    "        with open(nr_prox_out + '.tmp', 'w+') as f_sort:\n",
    "            sp.call(args, stdout=f_sort)\n",
    "        os.rename(nr_prox_out + '.tmp', nr_prox_out)\n",
    "        \n",
    "    ''' Save proximal x genome table '''\n",
    "    prox_table_out = output_dir + '/' + name + '_strain_by_' + side\n",
    "    prox_table_out = prox_table_out.replace('//','/')\n",
    "    prox_table_pickle = prox_table_out + '.pickle.gz'\n",
    "    prox_table_csv = prox_table_out + '.csv.gz'\n",
    "    print('Saving', prox_table_pickle, '...')\n",
    "    df_proximal.to_pickle(prox_table_pickle)\n",
    "    if save_csv:\n",
    "        print('Saving', prox_table_csv, '...')\n",
    "        df_proximal.to_csv(prox_table_csv)\n",
    "    return df_proximal\n",
    "\n",
    "    \n",
    "def consolidate_proximal(genome_proximals, nr_proximal_out, feature_to_allele, side):\n",
    "    ''' \n",
    "    Consolidates proximal sequences to a non-redudnant set with respect to each\n",
    "    gene described by feature_to_allele (maps_features to <name>_C#A#), then\n",
    "    creates a proximal x genome binary table. For use with fixed-length 3' or 5'UTRs.\n",
    "    Upstream features are <name>_C#U#, downstream features are <name>_C#D#.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    genome_proximals : list\n",
    "        List of paths to proximal sequences FNA to combine\n",
    "    nr_proximal_out : str\n",
    "        Path to output non-redundant proximal sequences as FNA\n",
    "    feature_to_allele : dict\n",
    "        Dictionary mapping headers to <name>_C#A# alleles\n",
    "    side : str\n",
    "        'upstream' or 'downstream' for 5'UTR or 3'UTR\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    df_proximal : pd.DataFrame\n",
    "         Binary proximal x genome table\n",
    "    '''\n",
    "    \n",
    "    ftype_abb = VARIANT_TYPES[side]\n",
    "    gene_to_unique_proximal = {} # maps gene:prox_seq:prox_seq_id (int)\n",
    "    genome_to_proximal = {} # maps genome:proximal_name:1 if present (<name>_C#U# or <name>_C#D#)\n",
    "    unique_proximal_ids = set() # record non-redundant list of proximal sequence IDs <name>_C#U# or <name>_C#D#\n",
    "    genome_order = [] # sorted list of genomes inferred from proximal sequence file names\n",
    "    \n",
    "    with open(nr_proximal_out, 'w+') as f_nr_prox:\n",
    "        for genome_proximal in sorted(genome_proximals):\n",
    "            ''' Infer genome name from genome filename '''\n",
    "            genome = genome_proximal.split('/')[-1] # trim off full path\n",
    "            genome = genome.split('_' + side)[0] # remove .fna footer\n",
    "            genome_to_proximal[genome] = {}\n",
    "            genome_order.append(genome)\n",
    "            \n",
    "            ''' Process genome's proximal record '''\n",
    "            with open(genome_proximal, 'r') as f_prox: # reading current proximal seq file\n",
    "                header = ''; prox_seq = ''; new_sequence = False\n",
    "                for line in f_prox.readlines(): # slight speed up reading whole file at once, should only be few MBs\n",
    "                    if line[0] == '>': # header line\n",
    "                        if len(prox_seq) > 0:\n",
    "                            ''' Process header-seq to non-redundant <name>_C#<U/D># proximal allele '''\n",
    "                            feature = header.split('_' + side + '(')[0] # trim off \"_<up/down>stream\" footer\n",
    "                            allele = feature_to_allele[feature] # get <name>_C#A# allele\n",
    "                            gene = __get_gene_from_allele__(allele) # gene <name>_C# gene\n",
    "                            if not gene in gene_to_unique_proximal:\n",
    "                                gene_to_unique_proximal[gene] = {}\n",
    "                            if not prox_seq in gene_to_unique_proximal[gene]:\n",
    "                                gene_to_unique_proximal[gene][prox_seq] = len(gene_to_unique_proximal[gene])\n",
    "                                prox_id = gene + ftype_abb + str(gene_to_unique_proximal[gene][prox_seq])\n",
    "                                unique_proximal_ids.add(prox_id)\n",
    "                                new_sequence = True\n",
    "                            prox_id = gene + ftype_abb + str(gene_to_unique_proximal[gene][prox_seq])\n",
    "                            genome_to_proximal[genome][prox_id] = 1\n",
    "                            \n",
    "                            ''' Write renamed sequence to running file '''\n",
    "                            if new_sequence:\n",
    "                                f_nr_prox.write('>' + prox_id + '\\n')\n",
    "                                f_nr_prox.write(prox_seq + '\\n')\n",
    "                                new_sequence = False\n",
    "\n",
    "                        header = line[1:].strip(); prox_seq = ''\n",
    "                    else: # sequence line\n",
    "                        prox_seq += line.strip()\n",
    "            \n",
    "                ''' Process last record'''\n",
    "                feature = header.split('_' + side + '(')[0] # trim off \"_<up/down>stream\" footer\n",
    "                allele = feature_to_allele[feature] # get <name>_C#A# allele\n",
    "                gene = __get_gene_from_allele__(allele) # gene <name>_C# gene\n",
    "                if not gene in gene_to_unique_proximal:\n",
    "                    gene_to_unique_proximal[gene] = {}\n",
    "                if not prox_seq in gene_to_unique_proximal[gene]:\n",
    "                    gene_to_unique_proximal[gene][prox_seq] = len(gene_to_unique_proximal[gene])\n",
    "                    prox_id = gene + ftype_abb + str(gene_to_unique_proximal[gene][prox_seq])\n",
    "                    unique_proximal_ids.add(prox_id)\n",
    "                    new_sequence = True\n",
    "                prox_id = gene + ftype_abb + str(gene_to_unique_proximal[gene][prox_seq])\n",
    "                genome_to_proximal[genome][prox_id] = 1\n",
    "\n",
    "                ''' Write renamed sequence to running file '''\n",
    "                if new_sequence:\n",
    "                    f_nr_prox.write('>' + prox_id + '\\n')\n",
    "                    f_nr_prox.write(prox_seq + '\\n')\n",
    "                    new_sequence = False\n",
    "                    \n",
    "    ''' Convert nested dict to dict of genome:SparseArrays once all proximal sequences are known '''\n",
    "    print('Sparsifying', side, 'table...')\n",
    "    prox_order = sorted(list(unique_proximal_ids))\n",
    "    del unique_proximal_ids\n",
    "    prox_indices = {prox_order[i]:i for i in range(len(prox_order))} # map proximal ID to index\n",
    "    for g,genome in enumerate(genome_order):\n",
    "        prox_array = np.zeros(shape=len(prox_order), dtype='int64')\n",
    "        for genome_prox in list(genome_to_proximal[genome].keys()):\n",
    "            prox_i = prox_indices[genome_prox]\n",
    "            prox_array[prox_i] = 1\n",
    "        genome_to_proximal[genome] = pd.arrays.SparseArray(prox_array)\n",
    "        genome_to_proximal[genome].fill_value = np.nan\n",
    "        \n",
    "    print('Constructing DataFrame...')\n",
    "    df_proximal = pd.DataFrame(data=genome_to_proximal, index=prox_order)\n",
    "    return df_proximal\n",
    "\n",
    "\n",
    "def extract_upstream_sequences(genome_gff, genome_fna, upstream_out, limits=(-50,3), max_overlap=-1,\n",
    "                               feature_to_allele=None, allele_names=None, include_fragments=False):\n",
    "    '''\n",
    "    Extracts nucleotides upstream of coding sequences to file, default 50bp + start codon.\n",
    "    Refer to extract_proximal_sequences() for parameters.\n",
    "    '''\n",
    "    extract_proximal_sequences(genome_gff, genome_fna, proximal_out=upstream_out, \n",
    "                               limits=limits, max_overlap=max_overlap, side='upstream', \n",
    "                               feature_to_allele=feature_to_allele, allele_names=allele_names,\n",
    "                               include_fragments=include_fragments)\n",
    "\n",
    "    \n",
    "def extract_downstream_sequences(genome_gff, genome_fna, downstream_out, limits=(-3,50), max_overlap=-1,\n",
    "                                 feature_to_allele=None, allele_names=None, include_fragments=False):\n",
    "    '''\n",
    "    Extracts nucleotides downstream of coding sequences to file, default stop codon + 50 bp.\n",
    "    Refer to extract_proximal_sequences() for parameters.\n",
    "    '''\n",
    "    extract_proximal_sequences(genome_gff, genome_fna, proximal_out=downstream_out, \n",
    "                               limits=limits, max_overlap=max_overlap, side='downstream', \n",
    "                               feature_to_allele=feature_to_allele, allele_names=allele_names,\n",
    "                               include_fragments=include_fragments)\n",
    "    \n",
    "                \n",
    "def extract_proximal_sequences(genome_gff, genome_fna, proximal_out, limits, max_overlap, side,\n",
    "                               feature_to_allele=None, allele_names=None, include_fragments=False):\n",
    "    '''\n",
    "    Extracts nucleotides upstream or downstream of coding sequences. \n",
    "    Interprets GFFs as formatted by PATRIC:\n",
    "        1) Assumes contigs are labeled \"accn|<contig>\". \n",
    "        2) Assumes protein features have \".peg.\" in the ID\n",
    "        3) Assumes ID = fig|<genome>.peg.#\n",
    "    Output features are named \"<feature header>_<up/down>stream(<limit1>,<limit2>)\" \n",
    "    if overlap is not restricted, otherwise features are named:\n",
    "    \"<feature header>_<up/down>stream(<limit1>,<limit2>,<max_overlap>)\"\n",
    "    Excludes features that do not have any UTR bases.\n",
    "        \n",
    "    Parameters\n",
    "    ----------\n",
    "    genome_gff : str\n",
    "        Path to genome GFF file with CDS coordinates\n",
    "    genome_fna : str\n",
    "        Path to genome FNA file with contig nucleotides\n",
    "    proximal_out : str\n",
    "        Path to output upstream/downstream sequences FNA files\n",
    "    limits : 2-tuple\n",
    "        Length of proximal region to extract, formatted (-X,Y). For upstream, extracts X \n",
    "        upstream bases (up to but excluding first base of start codon) and first Y coding \n",
    "        bases (including first base of start codon). For downstream, extracts the last X\n",
    "        coding bases and Y downstream bases. In both cases, the total length is X+Y bases.\n",
    "    max_overlap : int\n",
    "        If non-negative, truncates UTRs that cross over into coding sequences\n",
    "        of other genes such that the overlap is no more than <max_overlap> nts.\n",
    "        If negative, does not truncate UTRs s.t. all UTRs same length. \n",
    "        Note: Requires that GFF has features sorted by position.\n",
    "    side : str\n",
    "        'upstream' or 'downstream' for 5'UTR or 3'UTR\n",
    "    feature_to_allele : dict, str\n",
    "        Dictionary mapping original feature headers to <name>_C#A# short names,\n",
    "        alternatively, the allele_names file can be provided (default None)\n",
    "    allele_names : str\n",
    "        Path to allele names file if feature_to_allele is not provided,\n",
    "        should be named <name>_allele_names.tsv. If neither are provided,\n",
    "        simply processes all features present in the GFF (default None)\n",
    "    include_fragments : bool\n",
    "        If true, include upstream sequences that are not fully available \n",
    "        due to contig boundaries (default False)\n",
    "    '''\n",
    "\n",
    "    def extract_utr(start, stop, strand, contig, contig_seq, occupancy):\n",
    "        ''' Calculate ideal bounds for UTR, without accounting for overlap '''\n",
    "        pos = (side, strand)\n",
    "        utr_side = start if pos in [('upstream','+'),('downstream','-')] else stop # side UTR effectively starts from\n",
    "        utr_limits = limits if strand == '+' else (-limits[1], -limits[0]) # how to extend UTR bounds\n",
    "        utr_start = utr_side + utr_limits[0]\n",
    "        utr_stop = utr_side + utr_limits[1]\n",
    "        \n",
    "        ''' Optionally account for overlap '''\n",
    "        if max_overlap >= 0: # checking CDS-UTR overlaps\n",
    "            leftbound, rightbound = strand_occupancy[contig][strand][(start, stop)]\n",
    "            leftbound -= max_overlap\n",
    "            rightbound += max_overlap\n",
    "            if utr_start < leftbound: # 5' overlap exceeds limit\n",
    "                utr_start = leftbound\n",
    "                #print 'UTR start overlap for', start, stop\n",
    "            if utr_stop > rightbound: # 3' overlap exceeds limit\n",
    "                utr_stop = rightbound\n",
    "                #print 'UTR stop overlap for', start, stop\n",
    "            \n",
    "        ''' Extract UTR from computed bounds, RC if negative strand '''\n",
    "        proximal = contig_seq[utr_start:utr_stop].strip()\n",
    "        proximal = reverse_complement(proximal) if strand == '-' else proximal\n",
    "        is_fragment = (utr_start < 0) or (utr_stop > len(contig_seq)) # if cut-off by contig bounds\n",
    "        return proximal, is_fragment\n",
    "    \n",
    "    \n",
    "    strand_occupancy = {} # maps contig:strand:(start,stop):[(left start,stop), (right start,stop)]\n",
    "    if max_overlap >= 0:   \n",
    "        ''' Load feature order on each contig and each strand '''\n",
    "        occupancies = {}\n",
    "        with open(genome_gff, 'r') as f_gff:\n",
    "            for line in f_gff:\n",
    "                line = line.strip()\n",
    "                if len(line) > 0 and line[0] != '#':\n",
    "                    contig, src, feat_type, start, stop, score, \\\n",
    "                        strand, phase, attr_raw = line.split('\\t')\n",
    "                    if feat_type == 'CDS': # only consider CDS-UTR overlaps\n",
    "                        contig = contig.split('|')[-1] # accn|<contig> to just <contig>\n",
    "                        if not contig in occupancies:\n",
    "                            occupancies[contig] = {'+':[], '-':[]}\n",
    "                        start = int(start) - 1 # starts are 1-indexed\n",
    "                        stop = int(stop) # stops 1-indexed, inclusive = correct exclusive bound\n",
    "                        occupancies[contig][strand].append( (start, stop) )\n",
    "                         \n",
    "        ''' Convert feature order to 5'/3' neighbors pairs '''\n",
    "        for contig in occupancies:\n",
    "            strand_occupancy[contig] = {'+':{}, '-':{}}\n",
    "            for strand in occupancies[contig]:\n",
    "                n_features = len(occupancies[contig][strand])\n",
    "                for i, feature in enumerate(occupancies[contig][strand]): # features in order on strand\n",
    "                    leftbound = (-np.inf,-np.inf) if i == 0 else occupancies[contig][strand][i-1]\n",
    "                    rightbound = (np.inf,np.inf) if i == n_features-1 else occupancies[contig][strand][i+1]\n",
    "                    leftbound = leftbound[1] # take 3' end of nearest CDS to the 5' side\n",
    "                    rightbound = rightbound[0] # take 5' end of nearest CDS to the 3' end\n",
    "                    strand_occupancy[contig][strand][feature] = (leftbound, rightbound) \n",
    "        del occupancies\n",
    "                        \n",
    "    ''' Load contig sequences '''\n",
    "    contigs = load_sequences_from_fasta(genome_fna, header_fxn=lambda x: x.split()[0])\n",
    "            \n",
    "    ''' Load header-allele name mapping '''\n",
    "    if feature_to_allele: # dictionary provided directly\n",
    "        feat_to_allele = feature_to_allele\n",
    "    elif allele_names: # allele map file provided\n",
    "        feat_to_allele = __load_feature_to_allele__(allele_names)\n",
    "    else: # no allele mapping, process everything\n",
    "        feat_to_allele = None\n",
    "                    \n",
    "    ''' Parse GFF file for CDS coordinates '''\n",
    "    feature_footer = '_' + side\n",
    "    params = (limits[0], limits[1], max_overlap) if max_overlap >= 0 else limits\n",
    "    feature_footer += str(params).replace(' ','')\n",
    "    proximal_count = 0 # total UTRs extracted\n",
    "    coding_length = limits[1] if side == 'upstream' else -limits[0] # bases of UTR that overlap with reference gene CDS\n",
    "    with open(proximal_out, 'w+') as f_prox:\n",
    "        with open(genome_gff, 'r') as f_gff:\n",
    "            for line in f_gff:\n",
    "                line = line.strip()\n",
    "                if len(line) > 0 and line[0] != '#':\n",
    "                    contig, src, feat_type, start, stop, score, \\\n",
    "                        strand, phase, attr_raw = line.split('\\t')\n",
    "                    contig = contig.split('|')[-1] # accn|<contig> to just <contig>\n",
    "                    start = int(start) - 1 # starts are 1-indexed, inclusive\n",
    "                    stop = int(stop) # stops 1-indexed, inclusive = correct exclusive bound\n",
    "                    attrs = {} # key:value\n",
    "                    for entry in attr_raw.split(';'):\n",
    "                        k,v = entry.split('='); attrs[k] = v\n",
    "                    gffid = attrs['ID']\n",
    "\n",
    "                    ''' Verify allele has been mapped, and contig has been identified '''\n",
    "                    if contig in contigs: \n",
    "                        if gffid in feat_to_allele or feat_to_allele is None:\n",
    "                            try:\n",
    "                                contig_seq = contigs[contig]\n",
    "                            except KeyError:\n",
    "                                continue\n",
    "                            proximal, is_fragment = extract_utr(start, stop, strand, contig, contig_seq, strand_occupancy)\n",
    "                                \n",
    "                            ''' Save proximal sequence '''\n",
    "                            if len(proximal) > coding_length and (not is_fragment or include_fragments):\n",
    "                                feat_name = gffid + feature_footer\n",
    "                                f_prox.write('>' + feat_name + '\\n')\n",
    "                                f_prox.write(proximal + '\\n')\n",
    "                                proximal_count += 1\n",
    "                                \n",
    "    print('Loaded', side, 'sequences:', proximal_count)\n",
    "\n",
    "    \n",
    "def extract_noncoding(genome_gff, genome_fna, noncoding_out, flanking=(0,0),\n",
    "                      allowed_features=['transcript', 'tRNA', 'rRNA', 'misc_binding']):\n",
    "    '''\n",
    "    Extracts nucleotides for non-coding sequences. \n",
    "    Interprets GFFs as formatted by PATRIC:\n",
    "        1) Assumes contigs are labeled \"accn|<contig>\". \n",
    "        2) Assumes protein features have \".peg.\" in the ID\n",
    "        3) Assumes ID = fig|<genome>.peg.#\n",
    "        \n",
    "    Parameters\n",
    "    ----------\n",
    "    genome_gff : str\n",
    "        Path to genome GFF file with CDS coordinates\n",
    "    genome_fna : str\n",
    "        Path to genome FNA file with contig nucleotides\n",
    "    noncoding_out : str\n",
    "        Path to output transcript sequences FNA files\n",
    "    flanking : tuple\n",
    "        (X,Y) where X = number of nts to include from 5' end of feature,\n",
    "        and Y = number of nts to include from 3' end feature. Features\n",
    "        may be truncated by contig boundaries (default (0,0))\n",
    "    allowed_features : list\n",
    "        List of GFF feature types to extract. Default excludes \n",
    "        features labeled \"CDS\" or \"repeat_region\" \n",
    "        (default ['transcript', 'tRNA', 'rRNA', 'misc_binding'])\n",
    "    '''\n",
    "    contigs = load_sequences_from_fasta(genome_fna, header_fxn=lambda x:x.split()[0])\n",
    "    with open(noncoding_out, 'w+') as f_noncoding:\n",
    "        with open(genome_gff, 'r') as f_gff:\n",
    "            for line in f_gff:\n",
    "                ''' Check for non-comment and non-empty line '''\n",
    "                if not line[0] == '#' and not len(line.strip()) == 0: \n",
    "                    contig, src, feature_type, start, stop, \\\n",
    "                        score, strand, phase, meta = line.split('\\t')\n",
    "                    contig = contig[5:] # trim off \"accn|\" header\n",
    "                    start = int(start)\n",
    "                    stop = int(stop)\n",
    "                    \n",
    "                    if feature_type in allowed_features: \n",
    "                        ''' Get noncoding feature sequence and ID '''\n",
    "                        contig_seq = contigs[contig]\n",
    "                        fstart = start - 1 - flanking[0]\n",
    "                        fstart = max(0,fstart) # avoid looping due to contig boundaries\n",
    "                        fstop = stop + flanking[1]\n",
    "                        feature_seq = contig_seq[fstart:fstop]\n",
    "                        if strand == '-': # negative strand\n",
    "                            feature_seq = reverse_complement(feature_seq)\n",
    "                        meta_key_vals = [x.split('=') for x in meta.split(';')]\n",
    "                        metadata = {x[0]:x[1] for x in meta_key_vals}\n",
    "                        feature_id = metadata['ID']\n",
    "                        \n",
    "                        ''' Save to output file '''\n",
    "                        feature_seq = '\\n'.join(feature_seq[i:i+70] for i in range(0, len(feature_seq), 70))\n",
    "                        f_noncoding.write('>' + feature_id + '\\n')\n",
    "                        f_noncoding.write(feature_seq + '\\n') \n",
    "\n",
    "    \n",
    "def validate_gene_table(df_genes, df_alleles, log_group=1):\n",
    "    '''\n",
    "    Verifies that the gene x genome table is consistent with the\n",
    "    corresponding allele x genome table. Optimized to run column-by-column\n",
    "    rather than gene-by-gene for sparse tables.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df_genes : pd.DataFrame or str\n",
    "        Either the gene x genome table, or path to the table\n",
    "    df_alleles : pd.DataFrame or str\n",
    "        Either the allele x genome table, or path to the table\n",
    "    log_group : int\n",
    "        Print message per this many genomes \n",
    "    '''\n",
    "    dfg = load_feature_table(df_genes)\n",
    "    dfa = load_feature_table(df_alleles)\n",
    "    print('Validating gene clusters...')\n",
    "    num_inconsistencies = 0\n",
    "    for g,genome in enumerate(df_genes.columns):\n",
    "        if (g+1) % log_group == 0:\n",
    "            print(g+1, 'Testing', genome)\n",
    "        genes = set(dfg[genome].dropna().index)\n",
    "        alleles = dfa[genome].dropna().index\n",
    "        allele_genes = set(alleles.map(__get_gene_from_allele__))\n",
    "        if genes != allele_genes:\n",
    "            inconsistencies = genes.symmetric_difference(allele_genes)\n",
    "            print('\\tInconsistent:', inconsistencies)\n",
    "            num_inconsistencies += len(inconsistencies)\n",
    "    print('Gene Table Inconsistencies:', num_inconsistencies) \n",
    "\n",
    "\n",
    "def validate_gene_table_dense(df_genes, df_alleles):\n",
    "    '''\n",
    "    Verifies that the gene x genome table is consistent with the\n",
    "    corresponding allele x genome table. Original approach for\n",
    "    when df_genes and df_alleles are dense DataFrames.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df_genes : pd.DataFrame or str\n",
    "        Either the gene x genome table, or path to the table\n",
    "    df_alleles : pd.DataFrame or str\n",
    "        Either the allele x genome table, or path to the table\n",
    "    '''\n",
    "    dfg = load_feature_table(df_genes)\n",
    "    dfa = load_feature_table(df_alleles)\n",
    "    print('Validating gene clusters...')\n",
    "    \n",
    "    current_cluster = None; allele_data = []; \n",
    "    clusters_tested = 0; inconsistencies = 0\n",
    "    for allele_row in dfa.fillna(0).itertuples(name=None):\n",
    "        cluster = __get_gene_from_allele__(allele_row[0])\n",
    "        if current_cluster is None: # initializing\n",
    "            current_cluster = cluster\n",
    "        elif current_cluster != cluster: # end of gene cluster\n",
    "            alleles_all = np.array(allele_data)\n",
    "            has_gene = alleles_all.sum(axis=0) > 0\n",
    "            is_consistent = np.array_equal(has_gene, dfg.loc[current_cluster,:].fillna(0).values)\n",
    "            clusters_tested += 1\n",
    "            if not is_consistent:\n",
    "                print('Inconsistent', cluster)\n",
    "                print(has_gene)\n",
    "                print(dfg.loc[current_cluster,:].fillna(0).values)\n",
    "                inconsistencies += 1\n",
    "            if clusters_tested % 1000 == 0:\n",
    "                print('\\tTested', clusters_tested, 'clusters')\n",
    "            allele_data = []\n",
    "            current_cluster = cluster\n",
    "        allele_data.append(np.array(allele_row[1:]))\n",
    "    \n",
    "    ''' Process final line '''\n",
    "    alleles_all = np.array(allele_data) \n",
    "    has_gene = alleles_all.sum(axis=0) > 0\n",
    "    is_consistent = np.array_equal(has_gene, dfg.loc[current_cluster,:].fillna(0).values)\n",
    "    if not is_consistent:\n",
    "        print('Inconsistent', cluster)\n",
    "        print(has_gene)\n",
    "        print(dfg.loc[current_cluster,:].fillna(0).values)\n",
    "        inconsistencies += 1\n",
    "    print('Gene Table Inconsistencies:', inconsistencies)\n",
    "    \n",
    "\n",
    "def validate_upstream_table(df_upstream, upstream_fna_paths, nr_upstream_fna,\n",
    "                            allele_names, log_group=1):\n",
    "    '''\n",
    "    Verifies that the upstream x genome table is consistent with\n",
    "    the corresponding extracted upstream sequences. See \n",
    "    validate_table_against_fasta() for details.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df_upstream : pd.DataFrame\n",
    "        Binary upstream x genome table\n",
    "    upstream_fna_paths : list\n",
    "        List containing all paths to FNA files containing \n",
    "        upstream sequences for each genome\n",
    "    nr_upstream_fna : str\n",
    "        Path to FNA with non-redundant upstream sequences\n",
    "    allele_names : str\n",
    "        Path to allele names file generated by build_cds_pangenome()\n",
    "    log_group : int\n",
    "        Print message per this many genomes (default 1)\n",
    "    '''\n",
    "    return validate_table_against_fasta(\n",
    "        df_features=df_upstream, genome_fasta_paths=upstream_fna_paths, \n",
    "        features_fasta=nr_upstream_fna, allele_names=allele_names,\n",
    "        log_group=log_group)\n",
    "\n",
    "    \n",
    "def validate_downstream_table(df_downstream, downstream_fna_paths, nr_downstream_fna, \n",
    "                              allele_names, log_group=1):\n",
    "    '''\n",
    "    Verifies that the downstream x genome table is consistent with\n",
    "    the corresponding extracted downstream sequences. See \n",
    "    validate_table_against_fasta() for details.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df_downstream : pd.DataFrame\n",
    "        Binary downstream x genome table\n",
    "    downstream_fna_paths : list\n",
    "        List containing all paths to FNA files containing \n",
    "        downstream sequences for each genome\n",
    "    nr_downstream_fna : str\n",
    "        Path to FNA with non-redundant upstream sequences\n",
    "    allele_names : str\n",
    "        Path to allele names file generated by build_cds_pangenome()\n",
    "    log_group : int\n",
    "        Print message per this many genomes (default 1)\n",
    "    '''\n",
    "    return validate_table_against_fasta(\n",
    "        df_features=df_downstream, genome_fasta_paths=downstream_fna_paths, \n",
    "        features_fasta=nr_downstream_fna, allele_names=allele_names, \n",
    "        log_group=log_group)\n",
    "    \n",
    "    \n",
    "def validate_allele_table(df_alleles, genome_fasta_paths, \n",
    "                          alleles_fasta, log_group=1):\n",
    "    ''' \n",
    "    Verifies that the allele x genome table is consistent with the\n",
    "    the corresponding fasta files. Originally validate_table_against_fasta().\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df_alleles : pd.DataFrame\n",
    "        Binary allele x genome table, for either CDS or non-coding genes\n",
    "    genome_fasta_paths : list\n",
    "        List containing all paths to fastas containing feature sequences \n",
    "        per genome. FAAs for CDS, or FNAs for non-coding features.\n",
    "    alleles_fasta : str\n",
    "        Path to fasta with all non-redundant sequences\n",
    "    cluster_type : str\n",
    "        Either 'cds' or 'noncoding' depending on feature (default 'cds')\n",
    "    log_group : int\n",
    "        Print message per this many genomes (default 1)\n",
    "    '''\n",
    "    return validate_table_against_fasta(\n",
    "        df_features=df_alleles, genome_fasta_paths=genome_fasta_paths, \n",
    "        features_fasta=alleles_fasta, allele_names=None, log_group=log_group)\n",
    "    \n",
    "\n",
    "def validate_table_against_fasta(df_features, genome_fasta_paths, \n",
    "                                 features_fasta, allele_names=None, \n",
    "                                 log_group=1):\n",
    "    '''\n",
    "    Verifies that a table x genome table is consistent with the original \n",
    "    fasta files. Works for the following cases:\n",
    "    - CDS allele table vs original CDS FAA files\n",
    "    - CDS upstream table vs original upstream FNA files\n",
    "    - CDS downstream table vs original upstream FNA files\n",
    "    - Non-coding allele table vs original non-coding FNA files\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df_features : pd.DataFrame or str\n",
    "        Either the feature x genome table, or path to the table\n",
    "    genome_fasta_paths : list\n",
    "        Paths to genome fasta files originally combined and clustered\n",
    "    features_fasta : str\n",
    "        Path to non-redundant sequences corresponding to df_features\n",
    "    allele_names : str\n",
    "        Path to allele names file. If provided, verifies both sequence\n",
    "        and feature name at the cluster level. Required for upstream and \n",
    "        downstream sequence validation due to conserved UTRs (default None).\n",
    "    log_group : int\n",
    "        Print message per this many genomes (default 1)\n",
    "    '''\n",
    "    dfa = load_feature_table(df_features)\n",
    "    inconsistencies = 0 # number of genomes with table-genome inconsistencies\n",
    "    \n",
    "    ''' Pre-load allele names if available '''\n",
    "    if allele_names: \n",
    "        print('Loading feature names...')\n",
    "        feathash_to_allele = {}\n",
    "        with open(allele_names, 'r') as f:\n",
    "            for line in f:\n",
    "                data = line.strip().split('\\t')\n",
    "                allele = data[0]; features = data[1:]\n",
    "                for feature in features:\n",
    "                    ''' Workaround for PATRIC files: \n",
    "                        PATRIC gff files store features as fig|genome.peg.#, but\n",
    "                        PATRIC faa files store features as fig|genome.peg.#|locus_tag, \n",
    "                        whenever locus_tags are available. Will trim off locus_tag, \n",
    "                        but may possibly break compatibility with non-PATRIC files. '''\n",
    "                    if feature.count('|') == 2:\n",
    "                        feature = feature[:feature.rindex('|')]\n",
    "                    feature_hash = __hash_sequence__(feature)\n",
    "                    feathash_to_allele[feature_hash] = allele\n",
    "\n",
    "    ''' Pre-load hashes for non-redundant protein sequences '''\n",
    "    print('Loading non-redundant sequences...')\n",
    "    seqhash_to_feature = {}\n",
    "    def load_sequence_entry(seq_blocks, header):\n",
    "        if len(seq_blocks) > 0:\n",
    "            seq = ''.join(seq_blocks)\n",
    "            seq = seq if (allele_names is None) else seq + trim_variant(header)\n",
    "            seqhash = __hash_sequence__(seq)\n",
    "            if seqhash in seqhash_to_feature:\n",
    "                print('COLLISION:' , header)\n",
    "            seqhash_to_feature[seqhash] = header\n",
    "\n",
    "    with open(features_fasta, 'r') as f_fasta:\n",
    "        header = ''; seq_blocks = []\n",
    "        for line in f_fasta:\n",
    "            if line[0] == '>': # new sequence encountered\n",
    "                load_sequence_entry(seq_blocks, header)\n",
    "                header = line[1:].strip()\n",
    "                seq_blocks = []\n",
    "            else: # sequence encountered\n",
    "                seq_blocks.append(line.strip())\n",
    "        load_sequence_entry(seq_blocks, header) # process last record\n",
    "    print('Non-redundant sequences:', len(seqhash_to_feature))\n",
    "\n",
    "    ''' Validate individual genomes against table '''\n",
    "    \n",
    "    def check_genome_sequence(seq_blocks, genome_features, feature_name, num_missing):\n",
    "        if len(seq_blocks) > 0:\n",
    "            seq = ''.join(seq_blocks)\n",
    "            if not (allele_names is None):\n",
    "                ''' Also validating allele name '''\n",
    "                feature_name = feature_name.split('_upstream(')[0]\n",
    "                feature_name = feature_name.split('_downstream(')[0]\n",
    "                feature_hash = __hash_sequence__(feature_name)\n",
    "                if feature_hash in feathash_to_allele:\n",
    "                    seq += trim_variant(feathash_to_allele[feature_hash])\n",
    "            seqhash = __hash_sequence__(seq)\n",
    "            if seqhash in seqhash_to_feature:\n",
    "                ''' Note: Sequence hashes may be missing if any original\n",
    "                    sequences were excluded intentionally, i.e. too short '''\n",
    "                feature = seqhash_to_feature[seqhash] # original name to NR name\n",
    "                genome_features.add(feature)\n",
    "            else:\n",
    "                num_missing += 1\n",
    "        return genome_features\n",
    "    \n",
    "    missing_features = 0\n",
    "    feature_counts = dfa.sum() # genome x total features\n",
    "    for i, genome_fasta in enumerate(sorted(genome_fasta_paths)):\n",
    "        if (i+1) % log_group == 0:\n",
    "            print('Validating genome', i+1, ':', genome_fasta)\n",
    "        ''' Load all features present in the genome '''\n",
    "        genome_features = set()\n",
    "        with open(genome_fasta, 'r') as f_fasta:\n",
    "            feature_header = ''; seq_blocks = []\n",
    "            for line in f_fasta:\n",
    "                if line[0] == '>': # new sequence encountered\n",
    "                    genome_features = check_genome_sequence(seq_blocks, genome_features, feature_header, missing_features)\n",
    "                    feature_header = line[1:].strip()\n",
    "                    seq_blocks = []\n",
    "                else: # sequence encountered\n",
    "                    seq_blocks.append(line.strip())\n",
    "            genome_features = check_genome_sequence(seq_blocks, genome_features, feature_header, missing_features) \n",
    "            # process last record\n",
    "            \n",
    "        ''' Check that identified features are consistent with the table '''\n",
    "        genome = __get_genome_from_filename__(genome_fasta) # trim off full path and .fna/.faa\n",
    "        if not genome in dfa.columns: # possible footer\n",
    "            genome = '_'.join(genome.split('_')[:-1])\n",
    "        df_ga = dfa.loc[:,genome]\n",
    "        table_features = set(df_ga.index[df_ga == 1]) # features from df_features\n",
    "        test = table_features == genome_features # features from original fasta\n",
    "        inconsistencies += (1 - int(test))\n",
    "        if not test:\n",
    "            table_only = len(table_features.difference(genome_features))\n",
    "            genome_only = len(genome_features.difference(table_features))\n",
    "            print(genome, '\\t', 'Table only:', table_only, '\\t', 'Genome only:', genome_only)\n",
    "    print('Missing Features:', missing_features)\n",
    "    print('Feature Table Inconsistencies:', inconsistencies)\n",
    "\n",
    "\n",
    "def validate_upstream_table_direct(df_upstream, genome_fna_paths, nr_upstream_fna,\n",
    "                                  limits=(-50,3), log_group=1):\n",
    "    '''\n",
    "    Does a partial validation of the upstream x genome table by checking that\n",
    "    the recorded upstream sequences are present in the corresponding genome, \n",
    "    and counts start codons observed. DOES NOT check the exact location of the\n",
    "    upstream sequences. See validate_proximal_table() for parameters.\n",
    "    '''\n",
    "    validate_proximal_table(df_upstream, genome_fna_paths, nr_upstream_fna, \n",
    "                            limits, 'upstream', log_group)\n",
    "\n",
    "    \n",
    "def validate_downstream_table_direct(df_downstream, genome_fna_paths, nr_downstream_fna,\n",
    "                              limits=(-3,50), log_group=1):\n",
    "    '''\n",
    "    Does a partial validation of the downstream x genome table by checking that\n",
    "    the recorded downstream downstream are present in the corresponding genome, \n",
    "    and counts stop codons observed. DOES NOT check the exact location of the\n",
    "    downstream sequences. See validate_proximal_table() for parameters.\n",
    "    '''\n",
    "    validate_proximal_table(df_downstream, genome_fna_paths, nr_downstream_fna, \n",
    "                            limits, 'downstream', log_group)\n",
    "    \n",
    "\n",
    "def validate_proximal_table_direct(df_prox, genome_fna_paths, nr_prox_fna, limits, side, log_group=1):\n",
    "    '''\n",
    "    Does a partial validation of the proximal x genome table by checking that\n",
    "    the recorded proximal sequences are present in the corresponding genome, \n",
    "    and counts start codons observed. DOES NOT check the exact location of the\n",
    "    proximal sequences. TODO: Does not yet support non-fixed length UTRs.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df_prox : pd.DataFrame or str\n",
    "        Either the proximal x genome table, or path to the table as CSV or CSV.GZ\n",
    "    genome_fna_paths : list\n",
    "        Paths to FNAs for each genome's contigs\n",
    "    nr_prox_fna : str\n",
    "        Path to FNA with non-redundant proximal sequences per gene\n",
    "    limits : 2-tuple\n",
    "        Proximal sequence limits specified when extracting upstream \n",
    "        or downstream sequences (default (-50,3))\n",
    "    side : str\n",
    "        Either \"upstream\" or \"downstream\"\n",
    "    log_group : int\n",
    "        Print message per this many genomes \n",
    "    '''\n",
    "    dfp = load_feature_table(df_prox)\n",
    "    \n",
    "    ''' Load non-redundant proximal sequences '''\n",
    "    print('Loading', side, 'sequences...')\n",
    "    nr_prox = load_sequences_from_fasta(nr_prox_fna)\n",
    "            \n",
    "    ''' Verify present of each proximal sequence within each genome '''\n",
    "    window = limits[1] - limits[0]\n",
    "    for g, genome_fna in enumerate(genome_fna_paths):\n",
    "        genome_contigs = load_sequences_from_fasta(genome_fna)\n",
    "        genome = __get_genome_from_filename__(genome_fna)\n",
    "        if (g+1) % log_group == 0:\n",
    "            print(g+1, 'Evaluating', genome, genome_fna)\n",
    "        dfp_strain = dfp.loc[:,genome]\n",
    "        table_prox = dfp_strain.index[pd.notnull(dfp_strain)] # proximal sequences as defined by the table\n",
    "        table_prox_seqs = {nr_prox[x]:x for x in table_prox} # maps sequences to names\n",
    "        \n",
    "        ''' Scan all contigs for detected fixed-length proximal sequences '''\n",
    "        for contig in list(genome_contigs.values()):\n",
    "            for i in range(len(contig)):\n",
    "                segment = contig[i:i+window]\n",
    "                if segment in table_prox_seqs: \n",
    "                    table_prox_seqs.pop(segment)\n",
    "            rc_contig = reverse_complement(contig)\n",
    "            for i in range(len(rc_contig)):\n",
    "                segment = rc_contig[i:i+window]\n",
    "                if segment in table_prox_seqs: \n",
    "                    table_prox_seqs.pop(segment)\n",
    "                    \n",
    "        ''' Report undetected proximal sequences '''\n",
    "        for prox in table_prox_seqs:\n",
    "            print('\\tMissing', table_prox_seqs[prox], 'from', genome)\n",
    "        \n",
    "    ''' Count start/stop codons among non-redundant proximal sequences '''\n",
    "    if limits[1] >= 3 and side == 'upstream':\n",
    "        print('Computing start codon distribution...')\n",
    "        if limits[1] == 3:\n",
    "            get_start = lambda x: x[-3:]\n",
    "        else:\n",
    "            get_start = lambda x: x[-limits[1]:-limits[1]+3]\n",
    "        start_codons = list(map(get_start, list(nr_prox.values())))\n",
    "        print(collections.Counter(start_codons))\n",
    "    elif limits[0] <= -3 and side == 'downstream':\n",
    "        print('Computing stop codon distribution...')\n",
    "        if limits[0] == -3:\n",
    "            get_stop = lambda x: x[:3]\n",
    "        else:\n",
    "            get_stop = lambda x: x[-limits[0]-3:-limits[0]]\n",
    "        stop_codons = list(map(get_stop, list(nr_prox.values())))\n",
    "        print(collections.Counter(stop_codons))\n",
    "\n",
    "        \n",
    "def generate_annotations(features, annotation_files):\n",
    "    '''\n",
    "    For a set of features, creates a pd.Series with PATRIC annotations\n",
    "    for each feature, or np.nan if no annotation was found.\n",
    "    1) For cluster-level features, returns the consensus annotation\n",
    "    2) For variant-level features, returns the variant-specific \n",
    "        annotation if recorded as different from consensus. Otherwise,\n",
    "        returns the consensus annotation.\n",
    "    3) For proximal UTR features, returns the consensus annotation\n",
    "        of the parent cluster-level feature.\n",
    "        \n",
    "    Parameters\n",
    "    ----------\n",
    "    features : iterable\n",
    "        List of features to extract annotations.\n",
    "    annotations_files : iterable\n",
    "        Paths to files containing annotations, from extract_annotations().\n",
    "    '''\n",
    "    relevant_features = list(features) + []\n",
    "    for feature in features:\n",
    "        name, cluster_type, cluster_num, variant_type, variant_num = \\\n",
    "            breakdown_feature_name(feature)\n",
    "        if variant_type: # variant_level feature\n",
    "            feature_cluster = name + '_' + cluster_type + str(cluster_num)\n",
    "            relevant_features.append(feature_cluster)\n",
    "            \n",
    "    ''' Load relevant features from annotations file '''\n",
    "    relevant_features = set(relevant_features)\n",
    "    relevant_annotations = {}\n",
    "    for annot_file in annotation_files:\n",
    "        with open(annot_file, 'r') as f:\n",
    "            for line in f:\n",
    "                data = line.strip().split('\\t'); feature = data[0]\n",
    "                if feature in relevant_features:\n",
    "                    relevant_annotations[feature] = ';'.join(data[1:])\n",
    "    \n",
    "    ''' Process loaded annotations '''\n",
    "    feature_to_annot = {}\n",
    "    for feature in features:\n",
    "        if feature in relevant_annotations: # direct annotation exists\n",
    "            feature_to_annot[feature] = relevant_annotations[feature]\n",
    "        else: # possible cluster-level annotation exists\n",
    "            name, cluster_type, cluster_num, variant_type, variant_num = \\\n",
    "                breakdown_feature_name(feature)\n",
    "            feature_cluster = name + '_' + cluster_type + str(cluster_num)\n",
    "            if not variant_type is None and feature_cluster in relevant_annotations: \n",
    "                feature_to_annot[feature] = relevant_annotations[feature_cluster]\n",
    "            else: # cluster-level annotation is missing\n",
    "                feature_to_annot[feature] = np.nan\n",
    "    return pd.Series(feature_to_annot, index=features)\n",
    "\n",
    "\n",
    "\n",
    "def extract_coding_fna(genome_gff, genome_fna, coding_out,\n",
    "                       allowed_features=['CDS', 'tRNA']):\n",
    "    '''\n",
    "        Extracts nucleotides for coding sequences.\n",
    "        Interprets GFFs as formatted by PATRIC:\n",
    "            1) Assumes contigs are labeled \"accn|<contig>\".\n",
    "            2) Assumes protein features have \".peg.\" in the ID\n",
    "            3) Assumes ID = fig|<genome>.peg.#\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        genome_gff : str\n",
    "            Path to genome GFF file with CDS coordinates\n",
    "        genome_fna : str\n",
    "            Path to genome FNA file with contig nucleotides\n",
    "        coding_out : str\n",
    "            Path to output transcript sequences FNA files\n",
    "        allowed_features : list\n",
    "            List of GFF feature types to extract. Default includes\n",
    "            features labeled \"CDS\" and \"tRNA\"\n",
    "            (default ['CDS', 'tRNA'])\n",
    "    '''\n",
    "\n",
    "    contigs = load_sequences_from_fasta(genome_fna, header_fxn=lambda x: x.split()[0])\n",
    "    with open(coding_out, 'w+') as f_coding:\n",
    "        with open(genome_gff, 'r') as f_gff:\n",
    "            for line in f_gff:\n",
    "                ''' Check for non-comment and non-empty line '''\n",
    "                if not line[0] == '#' and not len(line.strip()) == 0:\n",
    "                    contig, src, feature_type, start, stop, \\\n",
    "                    score, strand, phase, meta = line.split('\\t')\n",
    "                    contig = contig[5:]  # trim off \"accn|\" header\n",
    "                    fstart = int(start) - 1\n",
    "                    fstop = int(stop)\n",
    "\n",
    "                    if feature_type in allowed_features:\n",
    "                        ''' Get noncoding feature sequence and ID '''\n",
    "                        try:\n",
    "                            contig_seq = contigs[contig]\n",
    "                        except KeyError:\n",
    "                            print(contigs.keys())\n",
    "                            break\n",
    "\n",
    "                        fstart = max(0, fstart)  # avoid looping due to contig boundaries\n",
    "                        feature_seq = contig_seq[fstart:fstop]\n",
    "                        if strand == '-':  # negative strand\n",
    "                            feature_seq = reverse_complement(feature_seq)\n",
    "                        meta_key_vals = [x.split('=') for x in meta.split(';')]\n",
    "                        metadata = {x[0]: x[1] for x in meta_key_vals}\n",
    "                        feature_id = metadata['ID']\n",
    "\n",
    "                        ''' Save to output file '''\n",
    "                        feature_seq = '\\n'.join(feature_seq[i:i + 70] for i in range(0, len(feature_seq), 70))\n",
    "                        f_coding.write('>' + feature_id + '\\n')\n",
    "                        f_coding.write(feature_seq + '\\n')\n",
    "\n",
    "\n",
    "\n",
    "def extract_annotations(genome_gffs, allele_name_file, annotations_out, \n",
    "                        batch=100, collapse_alleles=True, flexible_locus_tag=False,\n",
    "                        allowed_features=None):\n",
    "    '''\n",
    "    For a given allele name file (usually <org>_allele_names.tsv),\n",
    "    extracts the corresponding PATRIC annotations from original gff files.\n",
    "    Can load gff files in batches to limit memory usage.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    genome_gffs : list\n",
    "        List of paths to GFF files\n",
    "    allele_name_file : str\n",
    "        Path to file with allele-protein ID map, usually <org>_allele_names.tsv\n",
    "    annotations_out : str\n",
    "        Path to output annotations. Will also create a .tmp intermediate.\n",
    "    batch : int\n",
    "        Maximum GFF files to load into at once\n",
    "    collapse_alleles : bool\n",
    "        If True, reports annotations at the gene level, using the most common allele\n",
    "        annotation. Alleles with non-plurality annotation are reported separately.\n",
    "        If False, reports all unique annotations for all alleles. (default True)\n",
    "    flexible_locus_tag : bool\n",
    "        For PATRIC annotations, CDS features map to fig|<ID>|<locus_tag> \"3-term\" \n",
    "        when tags are available, while RNA features map to fig|<ID> only \"2-term\". \n",
    "        If flexible_locus_tag is True, then all annotations are mapped to both 3-term\n",
    "        and 2-term names when possible for better mapping at the cost of 2x memory use.\n",
    "        Otherwise, only one name will be mapped, preferring 3-term names (default False)\n",
    "    allowed_features : list or None\n",
    "        List of GFF feature types to extract. If None, all feature\n",
    "        types are extracted (default None)\n",
    "    '''\n",
    "    \n",
    "    ''' Copy allele name table '''\n",
    "    tmp_out = annotations_out + '.tmp'\n",
    "    shutil.copyfile(allele_name_file, tmp_out)\n",
    "    \n",
    "    ''' Iteratively replace protein IDs with annotations from GFFs (in batches) '''\n",
    "    n_gffs = len(genome_gffs)\n",
    "    for g in range(0,n_gffs,batch):\n",
    "        ''' Load annotations for batch of GFFs '''\n",
    "        annotations = {}\n",
    "        for gff in genome_gffs[g:g+batch]:\n",
    "            with open(gff,'r') as f_gff:\n",
    "                for line in f_gff:\n",
    "                    data = line.strip().split('\\t')\n",
    "                    if len(data) == 9: \n",
    "                        feature_type = data[2]\n",
    "                        if allowed_features is None or feature_type in allowed_features:\n",
    "                            line_annots = data[-1].split(';')\n",
    "                            line_annots = dict([x.split('=') for x in line_annots])\n",
    "                            if 'ID' in line_annots and 'product' in line_annots:\n",
    "                                product = line_annots['product']\n",
    "                                product = urllib.parse.unquote(product) # replace % hex characters\n",
    "                                fid2 = line_annots['ID']; fid3 = None\n",
    "                                if 'locus_tag' in line_annots:\n",
    "                                    fid3 = fid2 + '|' + line_annots['locus_tag']\n",
    "                                if flexible_locus_tag: # save both names when possible\n",
    "                                    annotations[fid2] = product\n",
    "                                    if not fid3 is None:\n",
    "                                        annotations[fid3] = product\n",
    "                                else: # save only 3-term, or only 2-term if no locus_tag \n",
    "                                    fid = fid2 if (fid3 is None) else fid3\n",
    "                                    annotations[fid] = product\n",
    "        print('Loaded', len(annotations), 'annotations from batch', g+1, '-', min(n_gffs,g+batch))\n",
    "        \n",
    "        ''' Incorporate newly loaded annotations '''\n",
    "        with open(tmp_out, 'r') as f_last:\n",
    "            with open(tmp_out+'2', 'w+') as f_next:\n",
    "                for line in f_last:\n",
    "                    data = line.strip().split('\\t')\n",
    "                    allele = data[0]; fids = data[1:]\n",
    "                    fids = [annotations[x] if x in annotations else x for x in fids]\n",
    "                    fids = list(collections.OrderedDict.fromkeys(fids)) # remove duplicate annotations\n",
    "                    output = allele + '\\t' + '\\t'.join(fids)\n",
    "                    f_next.write(output + '\\n')\n",
    "        shutil.move(tmp_out+'2', tmp_out) # remove last round\n",
    "\n",
    "    ''' Optionally collapse allele-level annotations to gene-level '''\n",
    "    if collapse_alleles:\n",
    "        with open(tmp_out, 'r') as f_last:\n",
    "            current_cluster = None\n",
    "            with open(annotations_out, 'w+') as f_next:\n",
    "                for line in f_last:\n",
    "                    data = line.strip().split('\\t')\n",
    "                    allele = data[0]\n",
    "                    cluster =  __get_gene_from_allele__(allele)\n",
    "                    allele_annots = '\\t'.join(data[1:])\n",
    "                    if current_cluster is None: # initialize first cluster\n",
    "                        current_cluster = cluster\n",
    "                        cluster_alleles = [allele]\n",
    "                        cluster_annots = [allele_annots]\n",
    "                    elif cluster == current_cluster: # continuing current cluster\n",
    "                        cluster_alleles.append(allele)\n",
    "                        cluster_annots.append(allele_annots)\n",
    "                    else: # start of new cluster\n",
    "                        most_common_annot, count = collections.Counter(cluster_annots).most_common(1)[0]\n",
    "                        f_next.write(current_cluster + '\\t' + most_common_annot + '\\n')\n",
    "                        for i, annots in enumerate(cluster_annots):\n",
    "                            if annots != most_common_annot:\n",
    "                                f_next.write(cluster_alleles[i] + '\\t' + annots + '\\n')\n",
    "                        # Initialize next cluster\n",
    "                        current_cluster = cluster\n",
    "                        cluster_alleles = [allele]\n",
    "                        cluster_annots = [allele_annots]\n",
    "        os.remove(tmp_out)\n",
    "    else: \n",
    "        shutil.move(tmp_out, annotations_out)\n",
    "    \n",
    "\n",
    "def load_sequences_from_fasta(fasta, header_fxn=None, seq_fxn=None, filter_fxn=None):\n",
    "    ''' Loads sequences from a FAA or FNA file into a dict\n",
    "        mapping headers to sequences. Can optionally apply a \n",
    "        function to all header (header_fxn) or to all\n",
    "        sequences (seq_fxn). Drops the \">\" from all headers,\n",
    "        and removes line breaks from sequences. '''\n",
    "    header_to_seq = {}\n",
    "    with open(fasta, 'r') as f:\n",
    "        header = ''; seq = ''\n",
    "        for line in f:\n",
    "            if line[0] == '>': # header line\n",
    "                if len(header) > 0 and len(seq) > 0:\n",
    "                    if filter_fxn is None or filter_fxn(header):\n",
    "                        seq = seq_fxn(seq) if seq_fxn else seq\n",
    "                        header_to_seq[header] = seq\n",
    "                header = line.strip()[1:]\n",
    "                header = header_fxn(header) if header_fxn else header\n",
    "                seq = ''\n",
    "            else: # sequence line\n",
    "                seq += line.strip()\n",
    "        if len(header) > 0 and len(seq) > 0:\n",
    "            if filter_fxn is None or filter_fxn(header):\n",
    "                seq = seq_fxn(seq) if seq_fxn else seq\n",
    "                header_to_seq[header] = seq\n",
    "    return header_to_seq\n",
    "\n",
    "\n",
    "def load_feature_table(feature_table):\n",
    "    ''' \n",
    "    Loads DataFrames from CSV, CSV.GZ, PICKLE, or PICKLE.GZ.\n",
    "    Uses index_col=0 for CSVs. Returns feature_table if provided \n",
    "    with anything other than a string.\n",
    "    '''\n",
    "    if type(feature_table) == str: # path provided\n",
    "        if feature_table[-4:].lower() == '.csv' or feature_table[-7:].lower() == '.csv.gz':\n",
    "            return pd.read_csv(feature_table, index_col=0)\n",
    "        elif feature_table[-7:].lower() == '.pickle' or feature_table[-10:].lower() == '.pickle.gz':\n",
    "            return pd.read_pickle(feature_table)\n",
    "        else:\n",
    "            return feature_table\n",
    "    else: # non-string input\n",
    "        return feature_table\n",
    "    \n",
    "    \n",
    "def reverse_complement(seq):\n",
    "    ''' Returns the reverse complement of a DNA sequence.\n",
    "        Supports lower/uppercase and ambiguous bases'''\n",
    "    return ''.join([DNA_COMPLEMENT[base] for base in list(seq)])[::-1]\n",
    "\n",
    "\n",
    "def create_feature_name(name, cluster_type, cluster_num, variant_type=None, variant_num=-1):\n",
    "    ''' \n",
    "    Creates the short name of a given feature, defined as \n",
    "    <name>_<cluster_type_short><cluster_num>_<variant_type_short><variant_num>\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    name : str\n",
    "        Header to precede short name\n",
    "    cluster_type : str\n",
    "        \"cds\" or \"noncoding\", abbreviated as C or T, respectively\n",
    "    cluster_num : int\n",
    "        ID of cluster\n",
    "    variant_type : str\n",
    "        \"allele\" or \"upstream\" or \"downstream\", abbreviated as A, U, or D,\n",
    "        respectively. If None, assumes feature refers to whole cluster (default None)\n",
    "    variant_num : int\n",
    "        ID of variant. If negative, assumes feature refers to whole cluster (default -1)\n",
    "    '''\n",
    "    if str(cluster_num) == '78264':\n",
    "        print('Create name:', cluster_num)\n",
    "    short_name = name + '_'\n",
    "    cluster_name_short = CLUSTER_TYPES[cluster_type]\n",
    "    short_name += cluster_name_short + str(cluster_num)\n",
    "    if (not variant_type is None) and (variant_num >= 0):\n",
    "        variant_type_short = VARIANT_TYPES[variant_type]\n",
    "        short_name += variant_type_short + str(variant_num)\n",
    "    return short_name\n",
    "\n",
    "\n",
    "def breakdown_feature_name(feature_name):\n",
    "    '''\n",
    "    Separates a feature name into the name, cluster-type, \n",
    "    cluster-number, variant-type, and variant-number.\n",
    "    Example 1: EsC_A123U56 -> ['EsC', 'A', 123, 'U', 56]\n",
    "    Example 2: PsA_T789 -> ['PsA', 'T', 89, None, None]\n",
    "    '''\n",
    "    data = feature_name.split('_')\n",
    "    name = '_'.join(data[:-1]); footer = data[-1]\n",
    "    cluster_type = footer[0]\n",
    "    for i in range(1,len(footer)): # identify variant type\n",
    "        if footer[i] in VARIANT_TYPES_REV:\n",
    "            variant_type = footer[i]\n",
    "            cluster_num = int(footer[1:i])\n",
    "            variant_num = int(footer[i+1:])\n",
    "            return name, cluster_type, cluster_num, variant_type, variant_num\n",
    "    cluster_num = int(footer[1:])\n",
    "    return name, cluster_type, cluster_num, None, None\n",
    "\n",
    "\n",
    "def trim_variant(feature_name):\n",
    "    ''' Removes allele/upstream/downstream variant label\n",
    "        to yield a cluster-level feature name. Trims off\n",
    "        from the right to the right-most alphabetic character.\n",
    "        Returns the input if no alphabetic characters are\n",
    "        encountered. '''\n",
    "    for i in range(1,len(feature_name)):\n",
    "        if feature_name[-i].isalpha():\n",
    "            return feature_name[:-i]\n",
    "    return feature_name\n",
    "\n",
    "\n",
    "def __create_sparse_data_frame__(sparse_array, index, columns):\n",
    "    ''' \n",
    "    Creates a SparseDataFrame from a scipy.sparse matrix by initializing \n",
    "    individual columns as SparseSeries, then concatenating them. \n",
    "    Sometimes faster than native SparseDataFrame initialization? Known issues: \n",
    "    \n",
    "    - Direct initialization is slow in v0.24.2, see https://github.com/pandas-dev/pandas/issues/16773\n",
    "    - Empty SparseDataFrame initialization time scales quadratically with number of columns\n",
    "    - Initializing as dense and converting sparse uses more memory than directly making sparse\n",
    "    '''\n",
    "    n_row, n_col = sparse_array.shape\n",
    "    X = sparse_array.T if n_row < n_col else sparse_array # make n_col < n_row\n",
    "    sparse_cols = []\n",
    "    for i in range(X.shape[1]): # create columns individually\n",
    "        sparse_col = pd.SparseSeries.from_coo(scipy.sparse.coo_matrix(X[:,i]), dense_index=True)\n",
    "        sparse_cols.append(sparse_col)\n",
    "    df = pd.concat(sparse_cols, axis=1)\n",
    "    df = df.T if n_row < n_col else df # transpose back if n_col > n_row originally\n",
    "    df.index = pd.Index(index)\n",
    "    df.columns = pd.Index(columns)\n",
    "    return df\n",
    "\n",
    "\n",
    "def __load_feature_to_allele__(allele_names):\n",
    "    ''' Loads feature-to-allele mapping from file, usually <name>_allele_names.tsv. '''\n",
    "    map_feature_to_gffid = lambda x: '|'.join(x.split('|')[:2])\n",
    "    feat_to_allele = {}\n",
    "    with open(allele_names, 'r') as f_all:\n",
    "        for line in f_all:\n",
    "            data = line.strip().split('\\t')\n",
    "            allele = data[0]; synonyms = data[1:]\n",
    "            for synonym in synonyms:\n",
    "                gff_synonym = map_feature_to_gffid(synonym)\n",
    "                feat_to_allele[gff_synonym] = allele\n",
    "    return feat_to_allele\n",
    "                          \n",
    "def __get_gene_from_allele__(allele):\n",
    "    ''' Converts <name>_C#A# or <name>_T#A# allele to \n",
    "        <name>_C# gene or <name>_T# transcript. '''\n",
    "    splitter = VARIANT_TYPES['allele']\n",
    "    return splitter.join(allele.split(splitter)[:-1])\n",
    "\n",
    "def __get_genome_from_filename__(filepath):\n",
    "    ''' Extracts genome from a filepath by removing the full \n",
    "        path and the extension '''\n",
    "    # return filepath.split('/')[-1][:-4] # old version\n",
    "    filename = os.path.split(filepath)[1] # remove full path\n",
    "    return os.path.splitext(filename)[0] # remove extension\n",
    "\n",
    "def __get_header_from_fasta_line__(line):\n",
    "    ''' Extracts a short header from a full header line in a fasta'''\n",
    "    return line.split()[0][1:].strip()\n",
    "\n",
    "def __hash_sequence__(seq):\n",
    "    ''' Hashes arbitary length strings/sequences to bytestrings '''\n",
    "    return hashlib.sha256(seq).digest()\n",
    "\n",
    "def __stream_stdout__(command):\n",
    "    ''' Hopefully Jupyter-safe method for streaming process stdout '''\n",
    "    process = sp.Popen(command, stdout=sp.PIPE, shell=True)\n",
    "    while True:\n",
    "        line = process.stdout.readline()\n",
    "        if not line:\n",
    "            break\n",
    "        yield line.rstrip()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb880d8-fc1d-4eae-89ef-16abd3c48862",
   "metadata": {},
   "source": [
    "## metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25956d0-3e0c-4046-a556-ed9fbf4a0e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mash_scrubbed_metadata = pd.read_csv('../../data/metadata/mash_scrubbed_species_metadata.csv', index_col=0, dtype='object')\n",
    "\n",
    "display(\n",
    "    mash_scrubbed_metadata.shape,\n",
    "    mash_scrubbed_metadata.head()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86076aed-6369-4f21-b114-a26ac780935f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of bakta-annotated faa files (needed for CD-HIT)\n",
    "\n",
    "BAKTA = '../../data/processed/bakta/'\n",
    "\n",
    "bakta_faa_paths = [\n",
    "    os.path.join(BAKTA, bakta_folder, bakta_folder+'.faa') \n",
    "    for bakta_folder in os.listdir(BAKTA)\n",
    "]\n",
    "\n",
    "bakta_faa_paths[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f5e878-90c0-4f97-8802-6259e68b6997",
   "metadata": {},
   "outputs": [],
   "source": [
    "'158836.803' in mash_scrubbed_metadata.genome_id.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa890ea-6eb0-493a-9e3d-f45267de38fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "to_remove = []\n",
    "for path in bakta_faa_paths:\n",
    "    if path.split('/')[-1][:-4] not in mash_scrubbed_metadata.genome_id.values:\n",
    "        to_remove.append(path)\n",
    "\n",
    "to_remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d4e268-a3dd-42de-b4b2-7edb42d78ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for element in to_remove:\n",
    "    bakta_faa_paths.remove(element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375e6822-303a-422a-a926-e6f60d9dd357",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "for path in tqdm(bakta_faa_paths):\n",
    "    assert os.path.isfile(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ee9788-20b9-4e1a-a56e-647caf02b44f",
   "metadata": {},
   "source": [
    "# Build CDS pangenome"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c588b8-ce3a-4e67-ac46-ddcd04c651fe",
   "metadata": {},
   "source": [
    "## 70% similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc33e915-5cb8-472e-b804-776c5e09cf92",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# NOTE: THIS CELL WILL TAKE ~1 HOUR TO RUN (on 40 cores) for 15k genomes\n",
    "\n",
    "# build CDS pangenome\n",
    "\n",
    "# Parameters used:\n",
    "# -c: cutoff for similarity (0.8)\n",
    "# -n: peptide seq used for similarity (=5 for -c between 0.7 to 1.0)\n",
    "# -aL: alignment to ensure gene segments aren't aligned improperly (0.8)\n",
    "# -T: threads to use (0 for using all available threads on system)\n",
    "# -M: memory to use (default is 800 MB, setting to 0 uses all available RAM)\n",
    "\n",
    "df_alleles, df_genes, header_to_allele = build_cds_pangenome(\n",
    "    genome_faa_paths=bakta_faa_paths,\n",
    "    output_dir='../../data/processed/cd-hit-results/sim70/',\n",
    "    name='Ebacter',\n",
    "    cdhit_args={'-n': 5, '-c':0.7, '-aL':0.8, '-T': 0, '-M': 0},\n",
    "    fastasort_path=None,\n",
    "    save_csv=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b258861-6520-44e9-b9ab-8065575b1e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(\n",
    "    f'gene matrix: {df_genes.shape}',\n",
    "    f'allele matrix: {df_alleles.shape}'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b038093-4ea5-42ea-b9f1-2e910c7d0361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For memory reasons, we delete these (they are already saved)\n",
    "\n",
    "print('deleting df_alleles from memory...')\n",
    "del df_alleles\n",
    "\n",
    "print('deleting df_genes from memory...')\n",
    "del df_genes\n",
    "\n",
    "print('finished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f129f0b5-9326-4145-b65b-213e5de7436c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.open('../../data/processed/cd-hit-results/header_to_allele_70.pickle.gz', 'wb') as f:\n",
    "    f.write(pickle.dumps(header_to_allele))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8defd130-bf1a-4203-a007-814c46cc5bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "del header_to_allele # Once saved, we delete this too"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92b5371-6a4f-4317-af6c-211c7df3e6f1",
   "metadata": {},
   "source": [
    "## 75% similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc11782b-f58b-4ade-856d-989472927b33",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# NOTE: THIS CELL WILL TAKE ~1 HOUR TO RUN (on 40 cores) for 15k genomes\n",
    "\n",
    "# build CDS pangenome\n",
    "\n",
    "# Parameters used:\n",
    "# -c: cutoff for similarity (0.8)\n",
    "# -n: peptide seq used for similarity (=5 for -c between 0.7 to 1.0)\n",
    "# -aL: alignment to ensure gene segments aren't aligned improperly (0.8)\n",
    "# -T: threads to use (0 for using all available threads on system)\n",
    "# -M: memory to use (default is 800 MB, setting to 0 uses all available RAM)\n",
    "\n",
    "df_alleles, df_genes, header_to_allele = build_cds_pangenome(\n",
    "    genome_faa_paths=bakta_faa_paths,\n",
    "    output_dir='../../data/processed/cd-hit-results/sim75/',\n",
    "    name='Ebacter',\n",
    "    cdhit_args={'-n': 5, '-c':0.75, '-aL':0.8, '-T': 0, '-M': 0},\n",
    "    fastasort_path=None,\n",
    "    save_csv=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0067b598-4d5c-4983-b9dc-ca83b28a84f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(\n",
    "    f'gene matrix: {df_genes.shape}',\n",
    "    f'allele matrix: {df_alleles.shape}'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758ba2c1-79bd-41bc-946d-9fb0cbee1b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For memory reasons, we delete these (they are already saved)\n",
    "print('deleting df_alleles from memory...')\n",
    "del df_alleles\n",
    "\n",
    "print('deleting df_genes from memory...')\n",
    "del df_genes\n",
    "\n",
    "print('finished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc487c7-e35f-4c14-9576-8a4ee12f5465",
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.open('../../data/processed/cd-hit-results/header_to_allele_75.pickle.gz', 'wb') as f:\n",
    "    f.write(pickle.dumps(header_to_allele))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bdb6c07-4e69-49af-bcb6-3639e1d22741",
   "metadata": {},
   "outputs": [],
   "source": [
    "del header_to_allele # Once saved, we delete this too"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0becca-c37e-45e7-a2ee-06d8fb398551",
   "metadata": {},
   "source": [
    "## 80% similarity (Default choice for most pangenomic analyses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2989dd3e-f492-437b-b718-fe1abb47e1c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# NOTE: THIS CELL WILL TAKE ~1 HOUR TO RUN (on 40 cores) for 15k genomes\n",
    "\n",
    "# build CDS pangenome\n",
    "\n",
    "# Parameters used:\n",
    "# -c: cutoff for similarity (0.8)\n",
    "# -n: peptide seq used for similarity (=5 for -c between 0.7 to 1.0)\n",
    "# -aL: alignment to ensure gene segments aren't aligned improperly (0.8)\n",
    "# -T: threads to use (0 for using all available threads on system)\n",
    "# -M: memory to use (default is 800 MB, setting to 0 uses all available RAM)\n",
    "\n",
    "df_alleles, df_genes, header_to_allele = build_cds_pangenome(\n",
    "    genome_faa_paths=bakta_faa_paths,\n",
    "    output_dir='../../data/processed/cd-hit-results/sim80/',\n",
    "    name='Ebacter',\n",
    "    cdhit_args={'-n': 5, '-c':0.8, '-aL':0.8, '-T': 0, '-M': 0, '-g': 1},\n",
    "    fastasort_path=None,\n",
    "    save_csv=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7920d86a-e353-4857-a8a1-1154a7291523",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(\n",
    "    f'gene matrix: {df_genes.shape}',\n",
    "    f'allele matrix: {df_alleles.shape}'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603096fe-97cc-4d9a-a842-f7151b74092f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71855978-5309-478c-8215-95acf89f9a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For memory reasons, we delete these (they are already saved)\n",
    "print('deleting df_alleles from memory...')\n",
    "del df_alleles\n",
    "\n",
    "print('deleting df_genes from memory...')\n",
    "del df_genes\n",
    "\n",
    "print('finished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3222e7c1-21af-4aaa-b11c-cb80d54cbd28",
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.open('../../data/processed/cd-hit-results/header_to_allele_80.pickle.gz', 'wb') as f:\n",
    "    f.write(pickle.dumps(header_to_allele))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0733434a-7703-4265-9e3f-59c33e109954",
   "metadata": {},
   "outputs": [],
   "source": [
    "del header_to_allele # Once saved, we delete this too"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e5f0e8-fae0-4e2f-81aa-08b70af1922a",
   "metadata": {},
   "source": [
    "## 85% similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379fb4f2-cc38-4a7f-85e7-58b961fe663d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# NOTE: THIS CELL WILL TAKE ~1 HOUR TO RUN (on 40 cores) for 15k genomes\n",
    "\n",
    "# build CDS pangenome\n",
    "\n",
    "# Parameters used:\n",
    "# -c: cutoff for similarity (0.8)\n",
    "# -n: peptide seq used for similarity (=5 for -c between 0.7 to 1.0)\n",
    "# -aL: alignment to ensure gene segments aren't aligned improperly (0.8)\n",
    "# -T: threads to use (0 for using all available threads on system)\n",
    "# -M: memory to use (default is 800 MB, setting to 0 uses all available RAM)\n",
    "\n",
    "df_alleles, df_genes, header_to_allele = build_cds_pangenome(\n",
    "    genome_faa_paths=bakta_faa_paths,\n",
    "    output_dir='../../data/processed/cd-hit-results/sim85/',\n",
    "    name='Ebacter',\n",
    "    cdhit_args={'-n': 5, '-c':0.85, '-aL':0.8, '-T': 0, '-M': 0},\n",
    "    fastasort_path=None,\n",
    "    save_csv=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3638e68-94b7-464f-9ac1-58850beea221",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(\n",
    "    f'gene matrix: {df_genes.shape}',\n",
    "    f'allele matrix: {df_alleles.shape}'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49447d5-f348-4ce5-9c28-a986535e4fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For memory reasons, we delete these (they are already saved)\n",
    "print('deleting df_alleles from memory...')\n",
    "del df_alleles\n",
    "\n",
    "print('deleting df_genes from memory...')\n",
    "del df_genes\n",
    "\n",
    "print('finished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07e261c-082f-4652-9e5f-7ff3ab23a416",
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.open('../../data/processed/cd-hit-results/header_to_allele_85.pickle.gz', 'wb') as f:\n",
    "    f.write(pickle.dumps(header_to_allele))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4e0d04-27e5-484e-9bea-c4599f337d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "del header_to_allele # Once saved, we delete this too"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4c461e-3863-419b-98c8-f56881b89419",
   "metadata": {},
   "source": [
    "## 90% similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8748d9b7-5f91-4cae-b61c-159a98bff332",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# NOTE: THIS CELL WILL TAKE ~1 HOUR TO RUN (on 40 cores) for 15k genomes\n",
    "\n",
    "# build CDS pangenome\n",
    "\n",
    "# Parameters used:\n",
    "# -c: cutoff for similarity (0.8)\n",
    "# -n: peptide seq used for similarity (=5 for -c between 0.7 to 1.0)\n",
    "# -aL: alignment to ensure gene segments aren't aligned improperly (0.8)\n",
    "# -T: threads to use (0 for using all available threads on system)\n",
    "# -M: memory to use (default is 800 MB, setting to 0 uses all available RAM)\n",
    "\n",
    "df_alleles, df_genes, header_to_allele = build_cds_pangenome(\n",
    "    genome_faa_paths=bakta_faa_paths,\n",
    "    output_dir='../../data/processed/cd-hit-results/sim90/',\n",
    "    name='Ebacter',\n",
    "    cdhit_args={'-n': 5, '-c':0.9, '-aL':0.8, '-T': 0, '-M': 0},\n",
    "    fastasort_path=None,\n",
    "    save_csv=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac91776-61b2-44e3-af02-ecbda3bdd3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(\n",
    "    f'gene matrix: {df_genes.shape}',\n",
    "    f'allele matrix: {df_alleles.shape}'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bdae2e2-7924-4bd9-bd9c-65be0a3677b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For memory reasons, we delete these (they are already saved)\n",
    "print('deleting df_alleles from memory...')\n",
    "del df_alleles\n",
    "\n",
    "print('deleting df_genes from memory...')\n",
    "del df_genes\n",
    "\n",
    "print('finished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677b1bd2-3f5e-4811-a494-23d98b4589a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.open('../../data/processed/cd-hit-results/header_to_allele_90.pickle.gz', 'wb') as f:\n",
    "    f.write(pickle.dumps(header_to_allele))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3510155-8d32-4772-86f2-528c4e675598",
   "metadata": {},
   "outputs": [],
   "source": [
    "del header_to_allele # Once saved, we delete this too"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b91988-e826-4818-b0f2-3b6f03addf1a",
   "metadata": {},
   "source": [
    "# Plot\n",
    "\n",
    "__This is mainly to see if we need to deviate from 80% sequence similarity or not__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46dfbd7-1daa-4938-a618-acca33ee3802",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [0.7, 0.75, 0.8, 0.85, 0.9]\n",
    "y = [113452, 119723, 128358, 141707, 165845]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(3,3))\n",
    "\n",
    "sns.lineplot(x=x, y=y, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d9124b-56e9-4800-8496-d93f000de823",
   "metadata": {},
   "source": [
    "__80%__ looks fine to use so we will stick with it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40876b8f-d774-4e47-94ce-3545a861d627",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pan-phylon]",
   "language": "python",
   "name": "conda-env-pan-phylon-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
